{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb913bf-acc7-4399-b8a4-2a4fcb1478c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "project_dir = '/blue/shenhaowang/qingqisong/be-and-active-travel'\n",
    "os.chdir(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169defed-0b00-42ac-a9d9-edcc91cf223b",
   "metadata": {},
   "source": [
    "This is the steps of feature engineering\n",
    "1.Travel related variables are only homed based ones.Data and Methods
3.1 Data Sources and Study Context
The analytic dataset was constructed by integrating six data sources covering the Chicago metropolitan area. The primary data source is the My Daily Travel survey administered by the Chicago Metropolitan Agency for Planning, which provides a relational database comprising four tables: household, person, location, and place (trip) files. The household file records dwelling characteristics including household size, vehicle ownership, income category, and homeownership status. The person file contains individual demographics such as age, sex, race, ethnicity, employment status, student enrollment, educational attainment, and driver license possession. The location file provides geocoded latitude and longitude coordinates for activity locations, with each respondent's home identified by a standardized location number code of 10000. The place file records every trip segment undertaken by each respondent on the designated travel day, including the travel mode, travel time in minutes, destination location identifier, and sequential position within the daily trip chain.

The second data source consists of fine-grained built environment metrics computed on a regular grid of 180-meter cells. Five indicators were calculated for each grid cell: intersection density, road network complexity, building density, land use diversity, and amenity density. Each grid cell record includes the latitude and longitude of the cell centroid along with the computed metric values.

The third data source is the U.S. Environmental Protection Agency Smart Location Database, which provides standardized built environment indicators at the census block group level. Three variables were extracted: gross population density on unprotected land (D1B), gross employment density on unprotected land (D1C), and an employment entropy index measuring the balance of employment across eight sectors (D2A_EPHHM).

The fourth data source is a pre-computed transit proximity file containing, for each household, the Euclidean distance in feet to the nearest rail station, the distance in feet to the nearest bus stop, and the count of bus stops within a quarter-mile radius of the home location.

The fifth data source is the official Central Business District boundary shapefile, which delineates the geographic extent of Chicago's CBD as a polygon feature.

The sixth data source is the Chicago Park District facilities shapefile, which contains point-geometry records representing the locations of park facilities across the city.

3.2 Dependent Variable Construction
Three dependent variables were constructed at the person level, each restricted exclusively to home-based trip segments to ensure spatial correspondence between the respondent's residential built environment and the observed travel behavior.

3.2.1 Home-Based Trip Identification
The identification of home-based trips proceeded as follows. The place file was sorted by household identifier, person identifier, and trip sequence number. A binary flag was created to indicate whether each trip segment terminates at the respondent's home, defined as the destination location number equaling 10000. A second binary flag was created to indicate whether each trip segment originates from home, operationalized by checking whether the immediately preceding trip in the respondent's daily sequence terminated at home. This was accomplished by applying a one-position lag to the home-destination flag within each person's trip chain. For the first recorded trip of each respondent's travel day, the home-origin flag was set to true under the assumption that the travel day begins at the residence. A trip segment was then classified as home-based if it either originates from home or terminates at home, yielding a composite binary indicator that combines both conditions through a logical disjunction.

3.2.2 MET-Weighted Physical Activity Minutes
The first dependent variable quantifies the total metabolic equivalent-weighted minutes of physical activity derived from active travel on home-based trips. For each trip segment classified as home-based, the travel time was multiplied by a mode-specific MET value. Walking trips, identified by mode code 101, received a MET multiplier of 3.3. Cycling trips, identified by mode codes 102, 103, and 104 representing personal bicycle, bicycle share, and other cycling modes respectively, received a MET multiplier of 6.0. All non-active mode trip segments received a MET value of zero. The person-level dependent variable was computed by summing MET-weighted minutes across all home-based trip segments for each respondent.

3.2.3 Transit Duration
The second dependent variable captures the total duration in minutes spent on public transit during home-based trips. Transit modes were identified by mode codes 500 through 506 and 509, encompassing local bus, rapid transit rail, commuter rail, intercity bus, intercity rail, shuttle bus, paratransit, and other public transit services. For each home-based trip segment using a transit mode, the reported travel time was retained; for all other segments the transit time was set to zero. The person-level variable was obtained by summing transit minutes across all home-based segments for each respondent.

3.2.4 Active Travel Duration
The third dependent variable measures the total unweighted duration in minutes of walking and cycling during home-based trips. Active modes were defined as mode codes 101, 102, 103, and 104. For each home-based trip segment using an active mode, the travel time was recorded; for all other segments the active travel time was set to zero. The person-level variable was computed by summing active travel minutes across all home-based trip segments.

3.2.5 Person-Level Aggregation
The trip-level data were aggregated to the person level by grouping on household and person identifiers and computing the sum of each dependent variable component. In addition to the three continuous dependent variables, the aggregation produced auxiliary count variables including the number of home-based walking trips, cycling trips, transit trips, and active trips per person, as well as the total number of home-based trips and the total number of all trips. Binary indicators were created to flag whether each respondent made at least one home-based active travel trip and at least one home-based transit trip on the travel day. Respondents who made no home-based trips of the relevant mode type received a value of zero for the corresponding dependent variable.

3.3 Independent Variable Construction
3.3.1 Person-Level Sociodemographic Variables
Individual-level covariates were derived from the person file. Age was retained as a continuous variable after coercing the raw field to numeric format. A binary indicator for female sex was created by testing whether the sex variable equaled 2. Driver license possession was coded as a binary indicator with a value of one when the license variable equaled 1. Employment status was coded as a binary indicator based on the employment ask variable, with a value of one when this variable equaled 1. Student status was coded as a binary indicator that takes the value of one if the work status variable equals 6, indicating student status, or if the student enrollment variable takes a value of 1 or 2, indicating full-time or part-time enrollment; the two conditions were combined through a logical disjunction to capture all students regardless of which survey item recorded their status.

Race was coded as a set of mutually exclusive binary indicators. White was coded as one when the race variable equaled 1. Black was coded as one when race equaled 2. Asian was coded as one when race equaled 3. Other race was coded as one when race took a positive value not equal to 1, 2, or 3. Hispanic ethnicity was captured by a separate binary indicator coded as one when the Hispanic origin variable equaled 1. Educational attainment was represented by a binary indicator for bachelor's degree or above, coded as one when the education variable was greater than or equal to 5.

3.3.2 Household-Level Variables
Household-level covariates were derived from the household file. Homeownership was coded as a binary indicator taking the value of one when the homeownership variable fell within the set of values 0, 1, and 2, which in the survey coding scheme correspond to various forms of ownership or purchase. A separate renter indicator was coded as one when the homeownership variable equaled 3. Household size was retained as a continuous variable. The number of household vehicles was retained as a continuous variable, and an additional binary indicator for zero-vehicle households was created by testing whether the vehicle count equaled zero. Household income was represented by a set of binary indicators: low income was coded as one when the income category variable equaled 1, medium income when it equaled 2 or 3, and high income when it was 4 or greater.

3.3.3 Transit Accessibility Variables
Transit proximity measures were extracted from the pre-computed transit file. The file was loaded and checked for the presence of three key variables: distance to the nearest rail station in feet, distance to the nearest bus stop in feet, and the count of bus stops within a quarter-mile radius. The join keys connecting this file to the person-level data were identified as household and person identifiers when both were present, or household identifier alone otherwise. Duplicate records on the join keys were removed by retaining the first occurrence. Distance values in feet were converted to miles by dividing by 5280. The transit variables were then held for subsequent merging with the master dataset.

3.3.4 Distance to the Central Business District
Distance from each respondent's home to the CBD was calculated using the official CBD boundary shapefile. The shapefile was loaded and reprojected to the Illinois State Plane East coordinate system (EPSG 3435, with distance units in feet). The individual polygon geometries within the shapefile were dissolved into a single unified boundary using a unary union operation.

Home locations were identified from the location file by filtering for records flagged as home locations. The latitude and longitude coordinates were validated to fall within the expected geographic bounds of the Chicago metropolitan area, defined as latitude between 41.5 and 42.5 degrees north and longitude between 88.5 and 87.0 degrees west. Valid home coordinates were converted to point geometries in the WGS84 geographic coordinate system and then reprojected to Illinois State Plane East to match the CBD boundary.

For each home location, the distance in feet to the CBD boundary was calculated as the Euclidean distance from the home point to the exterior boundary of the CBD polygon. A containment test was performed to determine whether each home point fell within the CBD polygon, and homes located inside the CBD were assigned a distance of zero. The distance in feet was converted to miles. One record per household was retained by removing duplicates on the household identifier.

3.3.5 Built Environment Metrics via KDTree Spatial Join
The five fine-grained built environment metrics were attached to home locations through a nearest-neighbor spatial join implemented using a KDTree data structure. The built environment file was loaded and validated for the presence of latitude, longitude, and the five metric columns: intersection density, road network complexity, building density, land use diversity, and amenity density.

Home locations were identified from the location file using the same procedure described for the CBD distance calculation. One home record per household was retained, and coordinates were validated against the Chicago metropolitan area bounds. The latitude and longitude values of all valid built environment grid centroids were assembled into a two-dimensional array, and a KDTree was constructed from these coordinates. For each home location, the KDTree was queried to find the index of the nearest grid centroid based on Euclidean distance in the latitude-longitude coordinate space. The five built environment metric values from the matched grid cell were then assigned to the household, with each variable receiving the prefix H_ to denote that it represents the home-location value. Diagnostic statistics on the matching distances were computed and reported, including the mean, minimum, and maximum distances in degrees between home locations and their nearest grid centroids.

3.3.6 EPA Smart Location Database Integration
The integration of EPA Smart Location Database variables required a two-stage spatial matching process. In the first stage, each household's home location was assigned to a census block group through a spatial point-in-polygon join. The Illinois state-level block group boundary shapefile from the TIGER/Line series was loaded and reprojected to Illinois State Plane East. The household home coordinates, already represented as a GeoDataFrame in the projected coordinate system from the CBD distance calculation, were spatially joined to the block group polygons using a "within" predicate, which tests whether each home point falls inside a block group polygon. The resulting join assigned a block group GEOID to each household.

In the second stage, the assigned GEOIDs were used to merge EPA Smart Location Database attributes. Both the block group GEOIDs from the spatial join and the GEOIDs in the EPA database were standardized to string format. Decimal suffixes were removed, leading zeros were restored through zero-padding to ensure consistent string length, and missing or null values were handled. The overlap between the two GEOID sets was assessed by computing the intersection of unique values. The EPA variables D1B, D1C, and D2A_EPHHM were then merged onto the household records using the standardized GEOID as the join key. Duplicate EPA records on the GEOID key were removed prior to merging by retaining the first occurrence.

3.3.7 Park Proximity
Distance to the nearest park facility was computed using a KDTree nearest-neighbor search. The Chicago Park District facilities shapefile was loaded and reprojected to Illinois State Plane East. Because the park data contain point geometries representing facility locations rather than polygon boundaries delineating park areas, only distance to the nearest facility was calculated; park area measures were not computed. The x and y coordinates of all park facility points were extracted and assembled into a two-dimensional array, from which a KDTree was constructed. For each household home location, the KDTree was queried to identify the nearest park facility and return the distance in feet, which was subsequently converted to miles.

3.4 Dataset Assembly
The final analytic dataset was assembled through a sequential left-merge process anchored on the person-level dependent variable file. The person-level outcomes file, containing the three dependent variables and auxiliary trip count variables for all respondents, served as the base table. Person-level sociodemographic features were merged using household and person identifiers as composite keys. Household-level features were merged using the household identifier alone. Transit accessibility variables were merged using either household and person identifiers or household identifier alone, depending on the granularity of the transit file. CBD distance variables were merged on the household identifier. Built environment features from the KDTree spatial join were merged on the household identifier. EPA Smart Location Database variables and park distance were added in a subsequent integration step using household and person identifiers.

Prior to each merge operation, all join key columns were standardized to a consistent numeric type by coercing values to numeric format, replacing any parsing failures with a sentinel value, and casting to 64-bit integer. This standardization ensured type compatibility across data sources that may have stored identifiers as strings, floats, or integers in the original files.

After all merges were completed, the three dependent variables were checked for missing values and any remaining nulls were filled with zero, reflecting the interpretation that respondents with no matched trip records had zero active travel or transit use. For all other numeric variables, missing values were filled with the column median as computed from the available observations.

3.5 Data Cleaning
The merged dataset was subjected to a systematic cleaning procedure to remove observations with invalid or missing values. The first cleaning step addressed negative values in the survey data, which represent various categories of nonresponse such as refused, don't know, or not applicable. All numeric columns other than the household and person identifiers and the block group GEOID were examined for negative values. Every negative value detected was replaced with a missing value indicator. The second step applied listwise deletion, removing all observations that contained at least one missing value across any numeric analysis variable. The number of observations removed and the retention rate were recorded.

Following cleaning, a verification check confirmed that the final dataset contained no missing values and no negative values in any numeric column. Summary statistics including the count, mean, standard deviation, minimum, median, and maximum were computed for all numeric variables in the clean dataset. The final clean dataset was saved as a comma-separated values file for use in subsequent modeling.

3.6 Variable Summary
The final analytic dataset contains variables organized into five conceptual groups. The dependent variables consist of total MET-weighted physical activity minutes from home-based active travel, total transit duration in minutes from home-based transit trips, and total active travel duration in minutes from home-based walking and cycling trips. The person-level sociodemographic variables include age, female indicator, driver license indicator, employment indicator, student indicator, race indicators for white, black, asian, and other race, hispanic ethnicity indicator, and bachelor's degree or above indicator. The household-level variables include homeownership indicator, renter indicator, household size, number of vehicles, zero-vehicle household indicator, and income category indicators. The built environment and accessibility variables include the five gridded morphological metrics at the home location, three EPA Smart Location Database indicators, distance to the nearest rail station, distance to the nearest bus stop, bus stop count within a quarter mile, distance to the CBD boundary, indicator for location inside the CBD, and distance to the nearest park facility. Auxiliary variables include home-based trip counts by mode, total trip counts, total travel time, and binary indicators for whether the respondent engaged in any home-based active travel or transit use on the survey day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434d90eb-3990-4554-b4b1-267d67ba4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Chicago Travel Behavior Feature Engineering Pipeline (Final Version)\n",
    "================================================================================\n",
    "FIXES:\n",
    "    1. BE metrics: Spatial join using KDTree (lat/lon matching)\n",
    "    2. CBD distance: From boundary shapefile\n",
    "    3. Correct variable coding (WKSTAT, HOMEOWN)\n",
    "    4. Join key type standardization\n",
    "    5. Home-Based (HB) trip filtering for dependent variables\n",
    "    6. Filter homes within Chicago city boundary\n",
    "    7. Drop rows with missing values (no imputation)\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a78202-2b35-425b-8427-f922cda5da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import KDTree\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cdc68c-e6be-48ba-8aa9-fa4c2957bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Central configuration.\"\"\"\n",
    "    \n",
    "    # FILE PATHS\n",
    "    PLACE_DF = './mydailytravel/place.csv'\n",
    "    PERSON_DF = './mydailytravel/person.csv'\n",
    "    HOUSEHOLD_DF = './mydailytravel/household.csv'\n",
    "    LOCATION_DF = './mydailytravel/location.csv'\n",
    "    BE_DF = '/blue/shenhaowang/qingqisong/GenerativeUrbanDesign-main/metrics_datagridc180/metrics_results.csv'\n",
    "    TRANSIT_FIXED_DF = './final_df_transit_fixed.csv'\n",
    "    CBD_SHP = './Central_Business_District_20260120/geo_export_48893cd7-ee67-4b97-afee-03338634fe3e.shp'\n",
    "    \n",
    "    # NEW: Chicago city boundary shapefile\n",
    "    CHICAGO_BOUNDARY_SHP = './Boundaries_City_20260209/geo_export_8fd7b0b2-d957-4597-87c2-aa49c13e4eeb.shp'\n",
    "    \n",
    "    OUTPUT_FILE = './city_homebased_chicago_research_ready_v2.csv'\n",
    "    \n",
    "    # CRS\n",
    "    CRS_WGS84 = 'EPSG:4326'\n",
    "    CRS_ILLINOIS_EAST = 'EPSG:3435'\n",
    "    \n",
    "    FEET_PER_MILE = 5280\n",
    "    \n",
    "    # MODE IDs\n",
    "    WALKING_MODE = 101\n",
    "    CYCLING_MODES = [102, 103, 104]\n",
    "    TRANSIT_MODES = [500, 501, 502, 503, 504, 505, 506, 509]\n",
    "    ACTIVE_MODES = [101, 102, 103, 104]\n",
    "    \n",
    "    # MET VALUES\n",
    "    MET_WALKING = 3.3\n",
    "    MET_CYCLING = 6.0\n",
    "    \n",
    "    # HOME LOCATION IDENTIFIER\n",
    "    HOME_LOCNO = 10000\n",
    "    \n",
    "    # VARIABLE CODING (CORRECTED)\n",
    "    WKSTAT_EMPLOYED = [0]\n",
    "    WKSTAT_RETIRED = [1]\n",
    "    WKSTAT_STUDENT = [6]\n",
    "    WKSTAT_UNEMPLOYED = [4, 5]\n",
    "    \n",
    "    SEX_FEMALE = 2\n",
    "    \n",
    "    RACE_WHITE = 1\n",
    "    RACE_BLACK = 2\n",
    "    RACE_ASIAN = 3\n",
    "    \n",
    "    HOMEOWN_OWN = [0, 1, 2]\n",
    "    HOMEOWN_RENT = [3]\n",
    "    \n",
    "    # BE FEATURE COLUMNS (exact names in BE file)\n",
    "    BE_FEATURE_COLS = [\n",
    "        'intersection_density',\n",
    "        'road_network_complexity', \n",
    "        'building_density',\n",
    "        'land_use_diversity',\n",
    "        'amenity_density'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b023e0a-5c3b-4831-b587-437889ca2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def print_section(title: str):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def print_step(step_num: int, description: str):\n",
    "    print(f\"\\n>>> STEP {step_num}: {description}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def load_csv(filepath: str, description: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV file with validation.\"\"\"\n",
    "    print(f\"  Loading {description}...\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"    ⚠ File not found: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    print(f\"    ✓ Loaded: {len(df):,} rows × {len(df.columns)} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_join_keys(df: pd.DataFrame, keys: list) -> pd.DataFrame:\n",
    "    \"\"\"Standardize join key columns to int64.\"\"\"\n",
    "    for key in keys:\n",
    "        if key in df.columns:\n",
    "            df[key] = pd.to_numeric(df[key], errors='coerce')\n",
    "            df[key] = df[key].fillna(-999).astype(np.int64)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4baa0c6-a73f-4cf8-839e-a4957082c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NEW: CHICAGO CITY BOUNDARY FILTER\n",
    "# ============================================================================\n",
    "\n",
    "def load_chicago_boundary():\n",
    "    \"\"\"Load Chicago city boundary shapefile.\"\"\"\n",
    "    print(\"  Loading Chicago city boundary...\")\n",
    "    \n",
    "    boundary_path = Config.CHICAGO_BOUNDARY_SHP\n",
    "    \n",
    "    # Try alternative paths if main path doesn't exist\n",
    "    if not os.path.exists(boundary_path):\n",
    "        alt_paths = [\n",
    "            './Boundaries_City_20260209/Boundaries_City.shp',\n",
    "            './Boundaries_City_20260209/geo_export*.shp',\n",
    "            './Boundaries_City.shp'\n",
    "        ]\n",
    "        for alt in alt_paths:\n",
    "            import glob\n",
    "            matches = glob.glob(alt)\n",
    "            if matches:\n",
    "                boundary_path = matches[0]\n",
    "                break\n",
    "    \n",
    "    if not os.path.exists(boundary_path):\n",
    "        print(f\"    ⚠ Chicago boundary shapefile not found: {boundary_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"    Loading: {boundary_path}\")\n",
    "    chicago_gdf = gpd.read_file(boundary_path)\n",
    "    \n",
    "    if chicago_gdf.crs is None:\n",
    "        chicago_gdf = chicago_gdf.set_crs(Config.CRS_WGS84)\n",
    "    chicago_gdf = chicago_gdf.to_crs(Config.CRS_ILLINOIS_EAST)\n",
    "    \n",
    "    chicago_boundary = chicago_gdf.geometry.unary_union\n",
    "    print(f\"    ✓ Chicago boundary loaded: {chicago_boundary.geom_type}\")\n",
    "    \n",
    "    return chicago_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20d4c6e-4e86-47b4-9d29-0697a5edce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_homes_within_chicago(location_df: pd.DataFrame, chicago_boundary) -> pd.DataFrame:\n",
    "    \"\"\"Filter home locations to only those within Chicago city boundary.\"\"\"\n",
    "    print(\"  Filtering homes within Chicago city boundary...\")\n",
    "    \n",
    "    if chicago_boundary is None:\n",
    "        print(\"    ⚠ No boundary available, skipping filter\")\n",
    "        return location_df\n",
    "    \n",
    "    df = location_df.copy()\n",
    "    \n",
    "    # Identify home locations\n",
    "    if 'loctype' in df.columns:\n",
    "        homes = df[df['loctype'] == 'Home'].copy()\n",
    "        if len(homes) == 0:\n",
    "            homes = df[df['loctype'].isin(['HOME', 'home', 1, '1'])].copy()\n",
    "    elif 'home' in df.columns:\n",
    "        homes = df[df['home'] == 1].copy()\n",
    "    else:\n",
    "        print(\"    ⚠ Cannot identify home locations\")\n",
    "        return location_df\n",
    "    \n",
    "    print(f\"    Total home locations: {len(homes):,}\")\n",
    "    \n",
    "    # Clean coordinates\n",
    "    homes['latitude'] = pd.to_numeric(homes['latitude'], errors='coerce')\n",
    "    homes['longitude'] = pd.to_numeric(homes['longitude'], errors='coerce')\n",
    "    \n",
    "    valid_coords = homes['latitude'].notna() & homes['longitude'].notna()\n",
    "    homes_valid = homes[valid_coords].copy()\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(homes_valid['longitude'], homes_valid['latitude'])]\n",
    "    homes_gdf = gpd.GeoDataFrame(homes_valid, geometry=geometry, crs=Config.CRS_WGS84)\n",
    "    homes_gdf = homes_gdf.to_crs(Config.CRS_ILLINOIS_EAST)\n",
    "    \n",
    "    # Check which homes are within Chicago boundary\n",
    "    homes_gdf['within_chicago'] = homes_gdf.geometry.apply(lambda pt: chicago_boundary.contains(pt))\n",
    "    \n",
    "    # Get sampno of homes within Chicago\n",
    "    homes_within = homes_gdf[homes_gdf['within_chicago']]['sampno'].unique()\n",
    "    homes_outside = homes_gdf[~homes_gdf['within_chicago']]['sampno'].unique()\n",
    "    \n",
    "    print(f\"    Homes within Chicago: {len(homes_within):,}\")\n",
    "    print(f\"    Homes outside Chicago: {len(homes_outside):,}\")\n",
    "    \n",
    "    # Filter original location_df to only include homes within Chicago\n",
    "    df_filtered = df[df['sampno'].isin(homes_within)].copy()\n",
    "    \n",
    "    print(f\"    ✓ Filtered location records: {len(df_filtered):,}\")\n",
    "    \n",
    "    return df_filtered, set(homes_within)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d458edc-157b-43b5-9fa6-9de1123bca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEPENDENT VARIABLE ENGINEERING (MODIFIED FOR HOME-BASED TRIPS)\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_dependent_variables(place_df: pd.DataFrame, valid_sampnos: set = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer THREE dependent variables for HOME-BASED TRIPS ONLY:\n",
    "    \n",
    "    A. Y_total_MET_minutes: MET-weighted physical activity (HB trips only)\n",
    "    B. Y_transit_duration_min: Time spent on public transit (HB trips only)\n",
    "    C. Y_active_duration: Total active travel duration (walk + bike, HB trips only)\n",
    "    \n",
    "    Home-Based Trip Definition:\n",
    "    - locno == 10000 represents the respondent's home\n",
    "    - HB trip = trip starting from home OR ending at home\n",
    "    \"\"\"\n",
    "    print(\"\\n  Identifying Home-Based (HB) Trips...\")\n",
    "    \n",
    "    df = place_df.copy()\n",
    "    \n",
    "    # Filter by valid sampnos if provided (Chicago boundary filter)\n",
    "    if valid_sampnos is not None:\n",
    "        before_count = len(df)\n",
    "        df = df[df['sampno'].isin(valid_sampnos)].copy()\n",
    "        print(f\"     Filtered to Chicago residents: {len(df):,} trips (from {before_count:,})\")\n",
    "    \n",
    "    df['travtime'] = pd.to_numeric(df['travtime'], errors='coerce').fillna(0).clip(lower=0)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 1: Identify Home-Based Segments\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    df['locno'] = pd.to_numeric(df['locno'], errors='coerce')\n",
    "    df['is_home_dest'] = (df['locno'] == Config.HOME_LOCNO)\n",
    "    \n",
    "    if 'plession' in df.columns:\n",
    "        df = df.sort_values(['sampno', 'perno', 'plession']).reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.sort_values(['sampno', 'perno']).reset_index(drop=True)\n",
    "    \n",
    "    df['is_home_orig'] = df.groupby(['sampno', 'perno'])['is_home_dest'].shift(1).fillna(False)\n",
    "    \n",
    "    first_trip_mask = ~df.duplicated(subset=['sampno', 'perno'], keep='first')\n",
    "    df.loc[first_trip_mask, 'is_home_orig'] = True\n",
    "    \n",
    "    df['is_hb_trip'] = (df['is_home_dest'] | df['is_home_orig'])\n",
    "    \n",
    "    total_trips = len(df)\n",
    "    hb_trips = df['is_hb_trip'].sum()\n",
    "    hb_ratio = hb_trips / total_trips * 100\n",
    "    print(f\"     Total trips: {total_trips:,}\")\n",
    "    print(f\"     Home-Based trips: {hb_trips:,} ({hb_ratio:.1f}%)\")\n",
    "    print(f\"       - Ending at home: {df['is_home_dest'].sum():,}\")\n",
    "    print(f\"       - Starting from home: {df['is_home_orig'].sum():,}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 2: Initialize columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df['MET_min'] = 0.0\n",
    "    df['transit_time'] = 0.0\n",
    "    df['is_active'] = 0\n",
    "    df['active_minutes'] = 0.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 3: Calculate MET Minutes (Active Travel) - HB TRIPS ONLY\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n  A. Calculating MET Minutes (Active Travel, HB only)...\")\n",
    "    \n",
    "    hb_walking_mask = (df['mode'] == Config.WALKING_MODE) & df['is_hb_trip']\n",
    "    df.loc[hb_walking_mask, 'MET_min'] = df.loc[hb_walking_mask, 'travtime'] * Config.MET_WALKING\n",
    "    \n",
    "    all_walking = (df['mode'] == Config.WALKING_MODE).sum()\n",
    "    hb_walking = hb_walking_mask.sum()\n",
    "    print(f\"     Walking (101): {hb_walking:,} HB trips / {all_walking:,} total | {df.loc[hb_walking_mask, 'travtime'].sum():,.0f} min\")\n",
    "    \n",
    "    hb_cycling_mask = (df['mode'].isin(Config.CYCLING_MODES)) & df['is_hb_trip']\n",
    "    df.loc[hb_cycling_mask, 'MET_min'] = df.loc[hb_cycling_mask, 'travtime'] * Config.MET_CYCLING\n",
    "    \n",
    "    all_cycling = df['mode'].isin(Config.CYCLING_MODES).sum()\n",
    "    hb_cycling = hb_cycling_mask.sum()\n",
    "    print(f\"     Cycling (102-104): {hb_cycling:,} HB trips / {all_cycling:,} total | {df.loc[hb_cycling_mask, 'travtime'].sum():,.0f} min\")\n",
    "    \n",
    "    hb_active_mask = (df['mode'].isin(Config.ACTIVE_MODES)) & df['is_hb_trip']\n",
    "    df.loc[hb_active_mask, 'is_active'] = 1\n",
    "    df.loc[hb_active_mask, 'active_minutes'] = df.loc[hb_active_mask, 'travtime']\n",
    "    \n",
    "    all_active = df['mode'].isin(Config.ACTIVE_MODES).sum()\n",
    "    hb_active = hb_active_mask.sum()\n",
    "    print(f\"     Active trips (HB): {hb_active:,} / {all_active:,} total ({hb_active/max(all_active,1)*100:.1f}%)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 4: Calculate Transit Duration - HB TRIPS ONLY\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n  B. Calculating Transit Duration (HB only)...\")\n",
    "    \n",
    "    hb_transit_mask = (df['mode'].isin(Config.TRANSIT_MODES)) & df['is_hb_trip']\n",
    "    df.loc[hb_transit_mask, 'transit_time'] = df.loc[hb_transit_mask, 'travtime']\n",
    "    \n",
    "    all_transit = df['mode'].isin(Config.TRANSIT_MODES).sum()\n",
    "    hb_transit = hb_transit_mask.sum()\n",
    "    print(f\"     Transit (HB): {hb_transit:,} / {all_transit:,} total | {df.loc[hb_transit_mask, 'travtime'].sum():,.0f} min\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 5: Aggregate by Person (HB trips only contribute to sums)\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n  C. Aggregating by Person (HB trips only)...\")\n",
    "    \n",
    "    df['is_hb_walking'] = ((df['mode'] == Config.WALKING_MODE) & df['is_hb_trip']).astype(int)\n",
    "    df['is_hb_cycling'] = ((df['mode'].isin(Config.CYCLING_MODES)) & df['is_hb_trip']).astype(int)\n",
    "    df['is_hb_transit'] = ((df['mode'].isin(Config.TRANSIT_MODES)) & df['is_hb_trip']).astype(int)\n",
    "    df['is_hb_active'] = ((df['mode'].isin(Config.ACTIVE_MODES)) & df['is_hb_trip']).astype(int)\n",
    "    df['hb_travtime'] = df['travtime'] * df['is_hb_trip'].astype(int)\n",
    "    \n",
    "    person_outcomes = df.groupby(['sampno', 'perno']).agg(\n",
    "        Y_total_MET_minutes=('MET_min', 'sum'),\n",
    "        Y_transit_duration_min=('transit_time', 'sum'),\n",
    "        Y_active_duration=('active_minutes', 'sum'),\n",
    "        n_walking_trips=('is_hb_walking', 'sum'),\n",
    "        n_cycling_trips=('is_hb_cycling', 'sum'),\n",
    "        n_transit_trips=('is_hb_transit', 'sum'),\n",
    "        n_active_trips=('is_hb_active', 'sum'),\n",
    "        n_hb_trips=('is_hb_trip', 'sum'),\n",
    "        n_total_trips=('mode', 'count'),\n",
    "        total_hb_travel_time_min=('hb_travtime', 'sum'),\n",
    "        total_travel_time_min=('travtime', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    person_outcomes['has_active_travel'] = (person_outcomes['n_active_trips'] > 0).astype(int)\n",
    "    person_outcomes['has_transit_travel'] = (person_outcomes['n_transit_trips'] > 0).astype(int)\n",
    "    person_outcomes['has_hb_trip'] = (person_outcomes['n_hb_trips'] > 0).astype(int)\n",
    "    \n",
    "    person_outcomes = standardize_join_keys(person_outcomes, ['sampno', 'perno'])\n",
    "    \n",
    "    numeric_cols = person_outcomes.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col not in ['sampno', 'perno']:\n",
    "            person_outcomes[col] = person_outcomes[col].fillna(0)\n",
    "    \n",
    "    print(f\"\\n  ✓ Outcomes: {len(person_outcomes):,} persons\")\n",
    "    print(f\"    Y_total_MET_minutes (HB): mean={person_outcomes['Y_total_MET_minutes'].mean():.2f}\")\n",
    "    print(f\"    Y_transit_duration_min (HB): mean={person_outcomes['Y_transit_duration_min'].mean():.2f}\")\n",
    "    print(f\"    Y_active_duration (HB): mean={person_outcomes['Y_active_duration'].mean():.2f}\")\n",
    "    print(f\"    Persons with HB active travel: {person_outcomes['has_active_travel'].sum():,}\")\n",
    "    print(f\"    Persons with HB transit travel: {person_outcomes['has_transit_travel'].sum():,}\")\n",
    "    print(f\"    Persons with any HB trip: {person_outcomes['has_hb_trip'].sum():,}\")\n",
    "    \n",
    "    return person_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b45263b-382d-42c6-b918-1b704d5db2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEMOGRAPHIC FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_person_features(person_df: pd.DataFrame, valid_sampnos: set = None) -> pd.DataFrame:\n",
    "    \"\"\"Create person-level demographic dummies with CORRECT coding.\"\"\"\n",
    "    print(\"  Creating person-level features...\")\n",
    "    \n",
    "    df = person_df.copy()\n",
    "    \n",
    "    # Filter by valid sampnos if provided\n",
    "    if valid_sampnos is not None:\n",
    "        df = df[df['sampno'].isin(valid_sampnos)].copy()\n",
    "        print(f\"    Filtered to Chicago residents: {len(df):,} persons\")\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    features['sampno'] = df['sampno']\n",
    "    features['perno'] = df['perno']\n",
    "    \n",
    "    if 'age' in df.columns:\n",
    "        features['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "    \n",
    "    if 'sex' in df.columns:\n",
    "        features['Female'] = (df['sex'] == Config.SEX_FEMALE).astype(int)\n",
    "        print(f\"    Female (sex==2): {features['Female'].sum():,} ({features['Female'].mean()*100:.1f}%)\")\n",
    "    else:\n",
    "        features['Female'] = 0\n",
    "    \n",
    "    if 'lic' in df.columns:\n",
    "        features['License'] = (df['lic'] == 1).astype(int)\n",
    "        print(f\"    License: {features['License'].sum():,}\")\n",
    "    else:\n",
    "        features['License'] = 0\n",
    "    \n",
    "    if 'emply_ask' in df.columns:\n",
    "        features['Employed'] = (df['emply_ask'] == 1).astype(int)\n",
    "        print(f\"    Employed (emply_ask==1): {features['Employed'].sum():,} ({features['Employed'].mean()*100:.1f}%)\")\n",
    "    else:\n",
    "        features['Employed'] = 0\n",
    "        print(\"    ⚠ 'emply_ask' column not found\")\n",
    "    \n",
    "    student_wkstat = df['wkstat'].isin(Config.WKSTAT_STUDENT) if 'wkstat' in df.columns else pd.Series([False]*len(df))\n",
    "    student_stude = df['stude'].isin([1, 2]) if 'stude' in df.columns else pd.Series([False]*len(df))\n",
    "    features['Student'] = (student_wkstat | student_stude).astype(int)\n",
    "    print(f\"    Student: {features['Student'].sum():,}\")\n",
    "    \n",
    "    if 'race' in df.columns:\n",
    "        features['White'] = (df['race'] == Config.RACE_WHITE).astype(int)\n",
    "        features['Black'] = (df['race'] == Config.RACE_BLACK).astype(int)\n",
    "        features['Asian'] = (df['race'] == Config.RACE_ASIAN).astype(int)\n",
    "        features['Other_Race'] = (~df['race'].isin([1, 2, 3]) & (df['race'] > 0)).astype(int)\n",
    "        print(f\"    White: {features['White'].sum():,} | Black: {features['Black'].sum():,} | Asian: {features['Asian'].sum():,}\")\n",
    "    else:\n",
    "        features['White'] = features['Black'] = features['Asian'] = features['Other_Race'] = 0\n",
    "    \n",
    "    if 'hisp' in df.columns:\n",
    "        features['Hispanic'] = (df['hisp'] == 1).astype(int)\n",
    "        print(f\"    Hispanic: {features['Hispanic'].sum():,}\")\n",
    "    else:\n",
    "        features['Hispanic'] = 0\n",
    "        \n",
    "    if 'educ' in df.columns:\n",
    "        features['Bachelor_Above'] = (df['educ'] >= 5).astype(int)\n",
    "        print(f\"    Bachelor_Above (educ>=5): {features['Bachelor_Above'].sum():,} ({features['Bachelor_Above'].mean()*100:.1f}%)\")\n",
    "    else:\n",
    "        features['Bachelor_Above'] = 0\n",
    "        print(\"    ⚠ 'educ' column not found\")\n",
    "    \n",
    "    features = standardize_join_keys(features, ['sampno', 'perno'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def engineer_household_features(household_df: pd.DataFrame, valid_sampnos: set = None) -> pd.DataFrame:\n",
    "    \"\"\"Create household-level features with CORRECT coding.\"\"\"\n",
    "    print(\"  Creating household-level features...\")\n",
    "    \n",
    "    df = household_df.copy()\n",
    "    \n",
    "    # Filter by valid sampnos if provided\n",
    "    if valid_sampnos is not None:\n",
    "        df = df[df['sampno'].isin(valid_sampnos)].copy()\n",
    "        print(f\"    Filtered to Chicago residents: {len(df):,} households\")\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    features['sampno'] = df['sampno']\n",
    "    \n",
    "    if 'homeown' in df.columns:\n",
    "        features['Home_Owner'] = df['homeown'].isin(Config.HOMEOWN_OWN).astype(int)\n",
    "        features['Renter'] = df['homeown'].isin(Config.HOMEOWN_RENT).astype(int)\n",
    "        print(f\"    Home_Owner (homeown∈[0,1,2]): {features['Home_Owner'].sum():,} ({features['Home_Owner'].mean()*100:.1f}%)\")\n",
    "    else:\n",
    "        features['Home_Owner'] = features['Renter'] = 0\n",
    "    \n",
    "    if 'hhsize' in df.columns:\n",
    "        features['hhsize'] = pd.to_numeric(df['hhsize'], errors='coerce').fillna(1)\n",
    "    else:\n",
    "        features['hhsize'] = 1\n",
    "    \n",
    "    if 'hhveh' in df.columns:\n",
    "        features['hhveh'] = pd.to_numeric(df['hhveh'], errors='coerce').fillna(0)\n",
    "        features['Zero_Vehicle_HH'] = (features['hhveh'] == 0).astype(int)\n",
    "        print(f\"    Zero_Vehicle_HH: {features['Zero_Vehicle_HH'].sum():,}\")\n",
    "    else:\n",
    "        features['hhveh'] = 0\n",
    "        features['Zero_Vehicle_HH'] = 0\n",
    "    \n",
    "    if 'hhinc' in df.columns:\n",
    "        features['hhinc'] = pd.to_numeric(df['hhinc'], errors='coerce')\n",
    "        features['Low_Income'] = (features['hhinc'] == 1).astype(int)\n",
    "        features['Med_Income'] = features['hhinc'].isin([2, 3]).astype(int)\n",
    "        features['High_Income'] = (features['hhinc'] >= 4).astype(int)\n",
    "    else:\n",
    "        features['hhinc'] = np.nan\n",
    "        features['Low_Income'] = features['Med_Income'] = features['High_Income'] = 0\n",
    "    \n",
    "    features = standardize_join_keys(features, ['sampno'])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c689c0-a5f6-4ac3-8010-34847fda7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SPATIAL FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def extract_transit_variables(transit_fixed_df: pd.DataFrame, valid_sampnos: set = None) -> pd.DataFrame:\n",
    "    \"\"\"Extract pre-computed transit proximity variables.\"\"\"\n",
    "    print(\"  Extracting transit variables from pre-computed file...\")\n",
    "    \n",
    "    if transit_fixed_df is None or len(transit_fixed_df) == 0:\n",
    "        print(\"    ⚠ No transit data\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = transit_fixed_df.copy()\n",
    "    \n",
    "    # Filter by valid sampnos if provided\n",
    "    if valid_sampnos is not None:\n",
    "        df = df[df['sampno'].isin(valid_sampnos)].copy()\n",
    "        print(f\"    Filtered to Chicago residents: {len(df):,} records\")\n",
    "    \n",
    "    required_cols = ['dist_rail_ft', 'dist_bus_ft', 'bus_count_14mile']\n",
    "    available = [col for col in required_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"    Available: {available}\")\n",
    "    \n",
    "    has_sampno = 'sampno' in df.columns\n",
    "    has_perno = 'perno' in df.columns\n",
    "    \n",
    "    if has_sampno and has_perno:\n",
    "        join_keys = ['sampno', 'perno']\n",
    "    elif has_sampno:\n",
    "        join_keys = ['sampno']\n",
    "    else:\n",
    "        print(\"    ⚠ No sampno column found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    cols_to_keep = join_keys + available\n",
    "    result = df[cols_to_keep].copy()\n",
    "    \n",
    "    result = standardize_join_keys(result, join_keys)\n",
    "    result = result.drop_duplicates(subset=join_keys, keep='first')\n",
    "    \n",
    "    if 'dist_rail_ft' in result.columns:\n",
    "        result['dist_rail_mi'] = result['dist_rail_ft'] / Config.FEET_PER_MILE\n",
    "        print(f\"    dist_rail: mean={result['dist_rail_ft'].mean():,.0f} ft\")\n",
    "    \n",
    "    if 'dist_bus_ft' in result.columns:\n",
    "        result['dist_bus_mi'] = result['dist_bus_ft'] / Config.FEET_PER_MILE\n",
    "        print(f\"    dist_bus: mean={result['dist_bus_ft'].mean():,.0f} ft\")\n",
    "    \n",
    "    if 'bus_count_14mile' in result.columns:\n",
    "        print(f\"    bus_count_14mile: mean={result['bus_count_14mile'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\n  ✓ Transit variables: {len(result):,} records\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_cbd_distance(location_df: pd.DataFrame, valid_sampnos: set = None) -> pd.DataFrame:\n",
    "    \"\"\"Calculate distance from Home to CBD BOUNDARY.\"\"\"\n",
    "    print(\"  Calculating CBD distance from boundary shapefile...\")\n",
    "    \n",
    "    cbd_path = Config.CBD_SHP\n",
    "    if not os.path.exists(cbd_path):\n",
    "        alt_paths = [\n",
    "            './Central_Business_District_20260120/geo_export_48893cd7-ee67-4b97-afee-03338634fe3e.shp',\n",
    "            './Central_Business_District.shp'\n",
    "        ]\n",
    "        for alt in alt_paths:\n",
    "            if os.path.exists(alt):\n",
    "                cbd_path = alt\n",
    "                break\n",
    "    \n",
    "    if not os.path.exists(cbd_path):\n",
    "        print(f\"    ⚠ CBD shapefile not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"    Loading: {cbd_path}\")\n",
    "    cbd_gdf = gpd.read_file(cbd_path)\n",
    "    \n",
    "    if cbd_gdf.crs is None:\n",
    "        cbd_gdf = cbd_gdf.set_crs(Config.CRS_WGS84)\n",
    "    cbd_gdf = cbd_gdf.to_crs(Config.CRS_ILLINOIS_EAST)\n",
    "    \n",
    "    cbd_boundary = cbd_gdf.geometry.unary_union\n",
    "    print(f\"    ✓ CBD boundary: {cbd_boundary.geom_type}\")\n",
    "    \n",
    "    df = location_df.copy()\n",
    "    \n",
    "    # Filter by valid sampnos if provided\n",
    "    if valid_sampnos is not None:\n",
    "        df = df[df['sampno'].isin(valid_sampnos)].copy()\n",
    "    \n",
    "    if 'loctype' in df.columns:\n",
    "        homes = df[df['loctype'] == 'Home'].copy()\n",
    "        if len(homes) == 0:\n",
    "            homes = df[df['loctype'].isin(['HOME', 'home', 1, '1'])].copy()\n",
    "    elif 'home' in df.columns:\n",
    "        homes = df[df['home'] == 1].copy()\n",
    "    else:\n",
    "        print(\"    ⚠ Cannot identify home locations\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"    Home locations: {len(homes):,}\")\n",
    "    \n",
    "    if len(homes) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    homes['latitude'] = pd.to_numeric(homes['latitude'], errors='coerce')\n",
    "    homes['longitude'] = pd.to_numeric(homes['longitude'], errors='coerce')\n",
    "    \n",
    "    valid = (\n",
    "        (homes['latitude'] >= 41.5) & (homes['latitude'] <= 42.5) &\n",
    "        (homes['longitude'] >= -88.5) & (homes['longitude'] <= -87.0) &\n",
    "        homes['latitude'].notna() & homes['longitude'].notna()\n",
    "    )\n",
    "    homes = homes[valid].copy()\n",
    "    print(f\"    Valid coordinates: {len(homes):,}\")\n",
    "    \n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(homes['longitude'], homes['latitude'])]\n",
    "    gdf = gpd.GeoDataFrame(homes, geometry=geometry, crs=Config.CRS_WGS84)\n",
    "    gdf = gdf.to_crs(Config.CRS_ILLINOIS_EAST)\n",
    "    \n",
    "    gdf['home_x'] = gdf.geometry.x\n",
    "    gdf['home_y'] = gdf.geometry.y\n",
    "    \n",
    "    print(\"    Calculating distances...\")\n",
    "    gdf['dist_cbd_ft'] = gdf.geometry.apply(lambda pt: pt.distance(cbd_boundary.boundary))\n",
    "    gdf['inside_cbd'] = gdf.geometry.apply(lambda pt: cbd_boundary.contains(pt)).astype(int)\n",
    "    gdf.loc[gdf['inside_cbd'] == 1, 'dist_cbd_ft'] = 0\n",
    "    gdf['dist_cbd_mi'] = gdf['dist_cbd_ft'] / Config.FEET_PER_MILE\n",
    "    \n",
    "    print(f\"    Homes inside CBD: {gdf['inside_cbd'].sum():,}\")\n",
    "    print(f\"    Mean distance (outside): {gdf.loc[gdf['inside_cbd'] == 0, 'dist_cbd_mi'].mean():.2f} mi\")\n",
    "    \n",
    "    gdf = gdf.drop_duplicates(subset=['sampno'], keep='first')\n",
    "    result = gdf[['sampno', 'home_x', 'home_y', 'dist_cbd_ft', 'dist_cbd_mi', 'inside_cbd']].copy()\n",
    "    \n",
    "    result = standardize_join_keys(result, ['sampno'])\n",
    "    \n",
    "    print(f\"\\n  ✓ CBD distance: {len(result):,} households\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902add3c-92ef-421c-821f-9febcc7a1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILT ENVIRONMENT - SPATIAL JOIN USING KDTREE\n",
    "# ============================================================================\n",
    "\n",
    "def process_built_environment_spatial(location_df: pd.DataFrame, \n",
    "                                       be_df: pd.DataFrame,\n",
    "                                       valid_sampnos: set = None) -> pd.DataFrame:\n",
    "    \"\"\"Spatial join: Attach BE metrics to home locations using KDTree.\"\"\"\n",
    "    print(\"  Performing spatial join for BE metrics using KDTree...\")\n",
    "    \n",
    "    if be_df is None or len(be_df) == 0:\n",
    "        print(\"    ⚠ No BE data available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"    BE data shape: {be_df.shape}\")\n",
    "    print(f\"    BE columns: {be_df.columns.tolist()}\")\n",
    "    \n",
    "    if 'lat' not in be_df.columns or 'lon' not in be_df.columns:\n",
    "        print(\"    ⚠ BE data missing 'lat' and 'lon' columns\")\n",
    "        print(f\"    Available columns: {be_df.columns.tolist()}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    available_features = [col for col in Config.BE_FEATURE_COLS if col in be_df.columns]\n",
    "    missing_features = [col for col in Config.BE_FEATURE_COLS if col not in be_df.columns]\n",
    "    \n",
    "    print(f\"    Available BE features: {available_features}\")\n",
    "    if missing_features:\n",
    "        print(f\"    Missing BE features: {missing_features}\")\n",
    "    \n",
    "    if not available_features:\n",
    "        print(\"    ⚠ No matching BE features found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = location_df.copy()\n",
    "    \n",
    "    # Filter by valid sampnos if provided\n",
    "    if valid_sampnos is not None:\n",
    "        df = df[df['sampno'].isin(valid_sampnos)].copy()\n",
    "    \n",
    "    if 'home' in df.columns:\n",
    "        home_locations = df[df['home'] == 1].copy()\n",
    "        print(f\"    Using 'home == 1' filter\")\n",
    "    elif 'loctype' in df.columns:\n",
    "        home_locations = df[df['loctype'] == 'Home'].copy()\n",
    "        if len(home_locations) == 0:\n",
    "            home_locations = df[df['loctype'].isin(['HOME', 'home', 1, '1'])].copy()\n",
    "        print(f\"    Using 'loctype == Home' filter\")\n",
    "    else:\n",
    "        print(\"    ⚠ Cannot identify home locations\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"    Total home location records: {len(home_locations):,}\")\n",
    "    \n",
    "    if 'latitude' in home_locations.columns and 'longitude' in home_locations.columns:\n",
    "        home_locations = home_locations.rename(columns={\n",
    "            'latitude': 'home_lat',\n",
    "            'longitude': 'home_lon'\n",
    "        })\n",
    "    else:\n",
    "        print(\"    ⚠ Home locations missing latitude/longitude\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    home_locations = home_locations.drop_duplicates(subset=['sampno'])\n",
    "    home_locations = home_locations[['sampno', 'home_lat', 'home_lon']].copy()\n",
    "    print(f\"    Unique households: {len(home_locations):,}\")\n",
    "    \n",
    "    home_locations['home_lat'] = pd.to_numeric(home_locations['home_lat'], errors='coerce')\n",
    "    home_locations['home_lon'] = pd.to_numeric(home_locations['home_lon'], errors='coerce')\n",
    "    \n",
    "    valid_homes = home_locations.dropna(subset=['home_lat', 'home_lon']).copy()\n",
    "    \n",
    "    valid_coords = (\n",
    "        (valid_homes['home_lat'] >= 41.5) & (valid_homes['home_lat'] <= 42.5) &\n",
    "        (valid_homes['home_lon'] >= -88.5) & (valid_homes['home_lon'] <= -87.0)\n",
    "    )\n",
    "    valid_homes = valid_homes[valid_coords].copy()\n",
    "    \n",
    "    invalid_count = len(home_locations) - len(valid_homes)\n",
    "    print(f\"    Valid home coordinates: {len(valid_homes):,}\")\n",
    "    if invalid_count > 0:\n",
    "        print(f\"    ⚠ Homes with invalid coordinates: {invalid_count:,}\")\n",
    "    \n",
    "    if len(valid_homes) == 0:\n",
    "        print(\"    ⚠ No valid home coordinates\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    be_df['lat'] = pd.to_numeric(be_df['lat'], errors='coerce')\n",
    "    be_df['lon'] = pd.to_numeric(be_df['lon'], errors='coerce')\n",
    "    \n",
    "    be_valid = be_df.dropna(subset=['lat', 'lon']).copy()\n",
    "    print(f\"    Valid BE grid points: {len(be_valid):,}\")\n",
    "    \n",
    "    if len(be_valid) == 0:\n",
    "        print(\"    ⚠ No valid BE coordinates\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    be_coords = be_valid[['lat', 'lon']].values\n",
    "    be_tree = KDTree(be_coords)\n",
    "    print(f\"    ✓ Built KDTree from {len(be_coords):,} BE grid points\")\n",
    "    \n",
    "    home_coords = valid_homes[['home_lat', 'home_lon']].values\n",
    "    \n",
    "    distances, indices = be_tree.query(home_coords)\n",
    "    \n",
    "    print(f\"    ✓ Nearest neighbor search completed\")\n",
    "    print(f\"      Distance stats (degrees):\")\n",
    "    print(f\"        Mean: {distances.mean():.6f}\")\n",
    "    print(f\"        Max:  {distances.max():.6f}\")\n",
    "    print(f\"        Min:  {distances.min():.6f}\")\n",
    "    \n",
    "    for col in available_features:\n",
    "        valid_homes[f'H_{col}'] = be_valid.iloc[indices][col].values\n",
    "    \n",
    "    h_columns = [f'H_{col}' for col in available_features]\n",
    "    \n",
    "    print(f\"    ✓ Attached {len(h_columns)} BE metrics to home locations\")\n",
    "    for col in h_columns:\n",
    "        non_null = valid_homes[col].notna().sum()\n",
    "        mean_val = valid_homes[col].mean()\n",
    "        print(f\"      → {col}: {non_null:,} non-null, mean={mean_val:.4f}\")\n",
    "    \n",
    "    home_with_be = home_locations.merge(\n",
    "        valid_homes[['sampno'] + h_columns],\n",
    "        on='sampno',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  ✓ Final home-BE dataset: {len(home_with_be):,} households\")\n",
    "    \n",
    "    result = home_with_be[['sampno'] + h_columns].copy()\n",
    "    result = standardize_join_keys(result, ['sampno'])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff9c05d-4f3d-4a0d-abb2-7ba8604517e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Execute the complete feature engineering pipeline.\"\"\"\n",
    "    \n",
    "    print_section(\"CHICAGO TRAVEL BEHAVIOR FEATURE ENGINEERING PIPELINE\")\n",
    "    print(f\"  Version: Final (with HB trip filtering + Chicago boundary)\")\n",
    "    print(f\"  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Load Data\n",
    "    # =========================================================================\n",
    "    print_step(1, \"Loading Data Sources\")\n",
    "    \n",
    "    place_df = load_csv(Config.PLACE_DF, \"Place/Trip Data\")\n",
    "    person_df = load_csv(Config.PERSON_DF, \"Person Data\")\n",
    "    household_df = load_csv(Config.HOUSEHOLD_DF, \"Household Data\")\n",
    "    location_df = load_csv(Config.LOCATION_DF, \"Location Data\")\n",
    "    transit_fixed_df = load_csv(Config.TRANSIT_FIXED_DF, \"Pre-computed Transit Data\")\n",
    "    \n",
    "    if os.path.exists(Config.BE_DF):\n",
    "        be_df = load_csv(Config.BE_DF, \"Built Environment Data\")\n",
    "    else:\n",
    "        print(f\"  ⚠ BE file not found: {Config.BE_DF}\")\n",
    "        be_df = pd.DataFrame()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Load Chicago Boundary and Filter Homes (NEW!)\n",
    "    # =========================================================================\n",
    "    print_step(2, \"Filtering Homes Within Chicago City Boundary\")\n",
    "    \n",
    "    chicago_boundary = load_chicago_boundary()\n",
    "    \n",
    "    if chicago_boundary is not None:\n",
    "        location_df_filtered, valid_sampnos = filter_homes_within_chicago(location_df, chicago_boundary)\n",
    "        print(f\"\\n  ✓ Valid households within Chicago: {len(valid_sampnos):,}\")\n",
    "    else:\n",
    "        print(\"  ⚠ Chicago boundary not available, using all homes\")\n",
    "        location_df_filtered = location_df\n",
    "        valid_sampnos = None\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Engineer Dependent Variables (HB TRIPS ONLY)\n",
    "    # =========================================================================\n",
    "    print_step(3, \"Engineering Dependent Variables (Home-Based Trips)\")\n",
    "    outcomes_df = engineer_dependent_variables(place_df, valid_sampnos)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Engineer Demographic Features\n",
    "    # =========================================================================\n",
    "    print_step(4, \"Engineering Demographic Features\")\n",
    "    person_features = engineer_person_features(person_df, valid_sampnos)\n",
    "    household_features = engineer_household_features(household_df, valid_sampnos)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Extract Transit Variables\n",
    "    # =========================================================================\n",
    "    print_step(5, \"Extracting Transit Variables\")\n",
    "    transit_vars = extract_transit_variables(transit_fixed_df, valid_sampnos)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: Calculate CBD Distance\n",
    "    # =========================================================================\n",
    "    print_step(6, \"Calculating CBD Distance\")\n",
    "    cbd_df = calculate_cbd_distance(location_df_filtered, valid_sampnos)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7: Spatial Join for BE Features\n",
    "    # =========================================================================\n",
    "    print_step(7, \"Spatial Join for Built Environment Features\")\n",
    "    be_features = process_built_environment_spatial(location_df_filtered, be_df, valid_sampnos)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 8: Merge All Data\n",
    "    # =========================================================================\n",
    "    print_step(8, \"Merging All Features\")\n",
    "    \n",
    "    final_df = outcomes_df.copy()\n",
    "    print(f\"\\n  Base (outcomes): {len(final_df):,} records\")\n",
    "    \n",
    "    print(\"\\n  Merging person_features [sampno, perno]...\")\n",
    "    final_df = final_df.merge(person_features, on=['sampno', 'perno'], how='left')\n",
    "    print(f\"    After: {len(final_df):,} records\")\n",
    "    \n",
    "    print(\"\\n  Merging household_features [sampno]...\")\n",
    "    final_df = final_df.merge(household_features, on='sampno', how='left')\n",
    "    print(f\"    After: {len(final_df):,} records\")\n",
    "    \n",
    "    if len(transit_vars) > 0:\n",
    "        join_keys = ['sampno', 'perno'] if 'perno' in transit_vars.columns else ['sampno']\n",
    "        print(f\"\\n  Merging transit_vars {join_keys}...\")\n",
    "        final_df = final_df.merge(transit_vars, on=join_keys, how='left')\n",
    "        print(f\"    After: {len(final_df):,} records\")\n",
    "        if 'dist_rail_ft' in final_df.columns:\n",
    "            non_null = final_df['dist_rail_ft'].notna().sum()\n",
    "            print(f\"    dist_rail_ft non-null: {non_null:,}\")\n",
    "    \n",
    "    if len(cbd_df) > 0:\n",
    "        print(\"\\n  Merging cbd_df [sampno]...\")\n",
    "        common = set(final_df['sampno'].unique()) & set(cbd_df['sampno'].unique())\n",
    "        print(f\"    Common sampno: {len(common):,}\")\n",
    "        final_df = final_df.merge(cbd_df, on='sampno', how='left')\n",
    "        print(f\"    After: {len(final_df):,} records\")\n",
    "        if 'dist_cbd_ft' in final_df.columns:\n",
    "            non_null = final_df['dist_cbd_ft'].notna().sum()\n",
    "            print(f\"    dist_cbd_ft non-null: {non_null:,}\")\n",
    "    \n",
    "    if len(be_features) > 0:\n",
    "        print(\"\\n  Merging be_features [sampno]...\")\n",
    "        common = set(final_df['sampno'].unique()) & set(be_features['sampno'].unique())\n",
    "        print(f\"    Common sampno: {len(common):,}\")\n",
    "        final_df = final_df.merge(be_features, on='sampno', how='left')\n",
    "        print(f\"    After: {len(final_df):,} records\")\n",
    "        \n",
    "        h_cols = [c for c in final_df.columns if c.startswith('H_')]\n",
    "        if h_cols:\n",
    "            first_h = h_cols[0]\n",
    "            non_null = final_df[first_h].notna().sum()\n",
    "            print(f\"    {first_h} non-null: {non_null:,}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: Handle Missing Values (MODIFIED: DROP ROWS WITH NaN)\n",
    "    # =========================================================================\n",
    "    print_step(9, \"Handling Missing Values (Dropping Rows with NaN)\")\n",
    "    \n",
    "    # Fill dependent variables with 0 (no transit/active travel is a valid zero)\n",
    "    for col in ['Y_total_MET_minutes', 'Y_transit_duration_min', 'Y_active_duration']:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = final_df[col].fillna(0)\n",
    "            print(f\"  {col}: NaN → 0\")\n",
    "    \n",
    "    # Count missing values before dropping\n",
    "    print(\"\\n  Missing values before dropping:\")\n",
    "    numeric_cols = final_df.select_dtypes(include=[np.number]).columns\n",
    "    cols_with_na = []\n",
    "    for col in numeric_cols:\n",
    "        na_count = final_df[col].isna().sum()\n",
    "        if na_count > 0:\n",
    "            print(f\"    {col}: {na_count:,} NaN\")\n",
    "            cols_with_na.append(col)\n",
    "    \n",
    "    # Drop rows with any NaN in important columns\n",
    "    before_drop = len(final_df)\n",
    "    \n",
    "    # Define columns that must not have NaN\n",
    "    critical_cols = [col for col in final_df.columns if col not in ['sampno', 'perno']]\n",
    "    critical_cols = [col for col in critical_cols if col in final_df.columns]\n",
    "    \n",
    "    final_df = final_df.dropna(subset=critical_cols)\n",
    "    \n",
    "    after_drop = len(final_df)\n",
    "    dropped_count = before_drop - after_drop\n",
    "    \n",
    "    print(f\"\\n  ✓ Dropped {dropped_count:,} rows with missing values\")\n",
    "    print(f\"    Before: {before_drop:,} rows\")\n",
    "    print(f\"    After:  {after_drop:,} rows\")\n",
    "    \n",
    "    # Verify no NaN remaining\n",
    "    remaining_na = final_df[numeric_cols].isna().sum().sum()\n",
    "    print(f\"    Remaining NaN: {remaining_na}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 10: Descriptive Statistics\n",
    "    # =========================================================================\n",
    "    print_step(10, \"Descriptive Statistics\")\n",
    "    \n",
    "    stat_cols = [col for col in final_df.select_dtypes(include=[np.number]).columns\n",
    "                 if col not in ['sampno', 'perno']]\n",
    "    \n",
    "    stats = final_df[stat_cols].describe().T\n",
    "    stats = stats[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "    stats.columns = ['N', 'Mean', 'Std', 'Min', 'Q1', 'Median', 'Q3', 'Max']\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"DESCRIPTIVE STATISTICS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\nObservations: {len(final_df):,} | Variables: {len(stat_cols)}\")\n",
    "    print(\"\\n\" + stats.round(4).to_string())\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 11: Save Output\n",
    "    # =========================================================================\n",
    "    print_step(11, \"Saving Output\")\n",
    "    \n",
    "    final_df.to_csv(Config.OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\n  ✓ Saved: {Config.OUTPUT_FILE}\")\n",
    "    print(f\"  ✓ Records: {len(final_df):,}\")\n",
    "    print(f\"  ✓ Variables: {len(final_df.columns)}\")\n",
    "    \n",
    "    print(\"\\n  VARIABLE LIST:\")\n",
    "    print(\"  \" + \"-\" * 40)\n",
    "    for i, col in enumerate(final_df.columns, 1):\n",
    "        print(f\"    {i:2d}. {col}\")\n",
    "    \n",
    "    print_section(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f83486-3cc3-476a-9a48-c41500dcf09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  CHICAGO TRAVEL BEHAVIOR FEATURE ENGINEERING PIPELINE\n",
      "======================================================================\n",
      "  Version: Final (with HB trip filtering + Chicago boundary)\n",
      "  Timestamp: 2026-02-10 20:30:41\n",
      "\n",
      ">>> STEP 1: Loading Data Sources\n",
      "--------------------------------------------------\n",
      "  Loading Place/Trip Data...\n",
      "    ✓ Loaded: 77,313 rows × 105 columns\n",
      "  Loading Person Data...\n",
      "    ✓ Loaded: 30,683 rows × 116 columns\n",
      "  Loading Household Data...\n",
      "    ✓ Loaded: 12,391 rows × 25 columns\n",
      "  Loading Location Data...\n",
      "    ✓ Loaded: 110,091 rows × 13 columns\n",
      "  Loading Pre-computed Transit Data...\n",
      "    ✓ Loaded: 30,683 rows × 19 columns\n",
      "  Loading Built Environment Data...\n",
      "    ✓ Loaded: 21,600 rows × 16 columns\n",
      "\n",
      ">>> STEP 2: Filtering Homes Within Chicago City Boundary\n",
      "--------------------------------------------------\n",
      "  Loading Chicago city boundary...\n",
      "    Loading: ./Boundaries_City_20260209/geo_export_8fd7b0b2-d957-4597-87c2-aa49c13e4eeb.shp\n",
      "    ✓ Chicago boundary loaded: MultiPolygon\n",
      "  Filtering homes within Chicago city boundary...\n",
      "    Total home locations: 12,392\n",
      "    Homes within Chicago: 4,550\n",
      "    Homes outside Chicago: 7,842\n",
      "    ✓ Filtered location records: 40,031\n",
      "\n",
      "  ✓ Valid households within Chicago: 4,550\n",
      "\n",
      ">>> STEP 3: Engineering Dependent Variables (Home-Based Trips)\n",
      "--------------------------------------------------\n",
      "\n",
      "  Identifying Home-Based (HB) Trips...\n",
      "     Filtered to Chicago residents: 28,448 trips (from 77,313)\n",
      "     Total trips: 28,448\n",
      "     Home-Based trips: 20,969 (73.7%)\n",
      "       - Ending at home: 13,254\n",
      "       - Starting from home: 13,711\n",
      "\n",
      "  A. Calculating MET Minutes (Active Travel, HB only)...\n",
      "     Walking (101): 3,286 HB trips / 5,954 total | 46,969 min\n",
      "     Cycling (102-104): 819 HB trips / 1,155 total | 20,041 min\n",
      "     Active trips (HB): 4,105 / 7,109 total (57.7%)\n",
      "\n",
      "  B. Calculating Transit Duration (HB only)...\n",
      "     Transit (HB): 4,331 / 5,611 total | 201,452 min\n",
      "\n",
      "  C. Aggregating by Person (HB trips only)...\n",
      "\n",
      "  ✓ Outcomes: 6,398 persons\n",
      "    Y_total_MET_minutes (HB): mean=43.02\n",
      "    Y_transit_duration_min (HB): mean=31.49\n",
      "    Y_active_duration (HB): mean=10.47\n",
      "    Persons with HB active travel: 1,957\n",
      "    Persons with HB transit travel: 2,476\n",
      "    Persons with any HB trip: 6,398\n",
      "\n",
      ">>> STEP 4: Engineering Demographic Features\n",
      "--------------------------------------------------\n",
      "  Creating person-level features...\n",
      "    Filtered to Chicago residents: 9,233 persons\n",
      "    Female (sex==2): 5,052 (54.7%)\n",
      "    License: 6,496\n",
      "    Employed (emply_ask==1): 5,969 (64.6%)\n",
      "    Student: 2,294\n",
      "    White: 5,808 | Black: 1,937 | Asian: 496\n",
      "    Hispanic: 1,335\n",
      "    Bachelor_Above (educ>=5): 5,131 (55.6%)\n",
      "  Creating household-level features...\n",
      "    Filtered to Chicago residents: 4,550 households\n",
      "    Home_Owner (homeown∈[0,1,2]): 1,608 (35.3%)\n",
      "    Zero_Vehicle_HH: 1,559\n",
      "\n",
      ">>> STEP 5: Extracting Transit Variables\n",
      "--------------------------------------------------\n",
      "  Extracting transit variables from pre-computed file...\n",
      "    Filtered to Chicago residents: 9,233 records\n",
      "    Available: ['dist_rail_ft', 'dist_bus_ft', 'bus_count_14mile']\n",
      "    dist_rail: mean=4,425 ft\n",
      "    dist_bus: mean=757 ft\n",
      "    bus_count_14mile: mean=9.5\n",
      "\n",
      "  ✓ Transit variables: 9,233 records\n",
      "\n",
      ">>> STEP 6: Calculating CBD Distance\n",
      "--------------------------------------------------\n",
      "  Calculating CBD distance from boundary shapefile...\n",
      "    Loading: ./Central_Business_District_20260120/geo_export_48893cd7-ee67-4b97-afee-03338634fe3e.shp\n",
      "    ✓ CBD boundary: Polygon\n",
      "    Home locations: 4,551\n",
      "    Valid coordinates: 4,551\n",
      "    Calculating distances...\n",
      "    Homes inside CBD: 285\n",
      "    Mean distance (outside): 4.92 mi\n",
      "\n",
      "  ✓ CBD distance: 4,550 households\n",
      "\n",
      ">>> STEP 7: Spatial Join for Built Environment Features\n",
      "--------------------------------------------------\n",
      "  Performing spatial join for BE metrics using KDTree...\n",
      "    BE data shape: (21600, 16)\n",
      "    BE columns: ['idx', 'lat', 'lon', 'building_density', 'building_footprint_ratio', 'avg_building_size', 'road_density_km', 'intersection_density', 'road_network_complexity', 'land_use_diversity', 'unique_land_uses', 'avg_land_use_area', 'avg_land_use_area_km2', 'amenity_density', 'avg_distance_to_amenities', 'nearest_amenity_distance']\n",
      "    Available BE features: ['intersection_density', 'road_network_complexity', 'building_density', 'land_use_diversity', 'amenity_density']\n",
      "    Using 'home == 1' filter\n",
      "    Total home location records: 4,681\n",
      "    Unique households: 4,550\n",
      "    Valid home coordinates: 4,550\n",
      "    Valid BE grid points: 21,600\n",
      "    ✓ Built KDTree from 21,600 BE grid points\n",
      "    ✓ Nearest neighbor search completed\n",
      "      Distance stats (degrees):\n",
      "        Mean: 0.001885\n",
      "        Max:  0.003280\n",
      "        Min:  0.000114\n",
      "    ✓ Attached 5 BE metrics to home locations\n",
      "      → H_intersection_density: 4,550 non-null, mean=383.2421\n",
      "      → H_road_network_complexity: 4,550 non-null, mean=2.8719\n",
      "      → H_building_density: 4,550 non-null, mean=0.3441\n",
      "      → H_land_use_diversity: 4,550 non-null, mean=1.0403\n",
      "      → H_amenity_density: 4,550 non-null, mean=2.6691\n",
      "\n",
      "  ✓ Final home-BE dataset: 4,550 households\n",
      "\n",
      ">>> STEP 8: Merging All Features\n",
      "--------------------------------------------------\n",
      "\n",
      "  Base (outcomes): 6,398 records\n",
      "\n",
      "  Merging person_features [sampno, perno]...\n",
      "    After: 6,398 records\n",
      "\n",
      "  Merging household_features [sampno]...\n",
      "    After: 6,398 records\n",
      "\n",
      "  Merging transit_vars ['sampno', 'perno']...\n",
      "    After: 6,398 records\n",
      "    dist_rail_ft non-null: 6,398\n",
      "\n",
      "  Merging cbd_df [sampno]...\n",
      "    Common sampno: 3,424\n",
      "    After: 6,398 records\n",
      "    dist_cbd_ft non-null: 6,398\n",
      "\n",
      "  Merging be_features [sampno]...\n",
      "    Common sampno: 3,424\n",
      "    After: 6,398 records\n",
      "    H_intersection_density non-null: 6,398\n",
      "\n",
      ">>> STEP 9: Handling Missing Values (Dropping Rows with NaN)\n",
      "--------------------------------------------------\n",
      "  Y_total_MET_minutes: NaN → 0\n",
      "  Y_transit_duration_min: NaN → 0\n",
      "  Y_active_duration: NaN → 0\n",
      "\n",
      "  Missing values before dropping:\n",
      "\n",
      "  ✓ Dropped 0 rows with missing values\n",
      "    Before: 6,398 rows\n",
      "    After:  6,398 rows\n",
      "    Remaining NaN: 0\n",
      "\n",
      ">>> STEP 10: Descriptive Statistics\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "DESCRIPTIVE STATISTICS\n",
      "====================================================================================================\n",
      "\n",
      "Observations: 6,398 | Variables: 49\n",
      "\n",
      "                                N          Mean         Std           Min            Q1        Median            Q3           Max\n",
      "Y_total_MET_minutes        6398.0  4.302030e+01    117.7632  0.000000e+00  0.000000e+00  0.000000e+00  3.300000e+01  3.102000e+03\n",
      "Y_transit_duration_min     6398.0  3.148670e+01     56.3514  0.000000e+00  0.000000e+00  0.000000e+00  5.600000e+01  9.600000e+02\n",
      "Y_active_duration          6398.0  1.047360e+01     27.0543  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+01  6.290000e+02\n",
      "n_walking_trips            6398.0  5.136000e-01      1.0233  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.100000e+01\n",
      "n_cycling_trips            6398.0  1.280000e-01      0.5407  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  6.000000e+00\n",
      "n_transit_trips            6398.0  6.769000e-01      0.9475  0.000000e+00  0.000000e+00  0.000000e+00  2.000000e+00  8.000000e+00\n",
      "n_active_trips             6398.0  6.416000e-01      1.1452  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.100000e+01\n",
      "n_hb_trips                 6398.0  3.277400e+00      1.4743  1.000000e+00  3.000000e+00  3.000000e+00  4.000000e+00  1.300000e+01\n",
      "n_total_trips              6398.0  4.446400e+00      2.3900  1.000000e+00  3.000000e+00  4.000000e+00  6.000000e+00  1.900000e+01\n",
      "total_hb_travel_time_min   6398.0  6.649050e+01     63.2614  0.000000e+00  2.700000e+01  5.800000e+01  9.200000e+01  9.650000e+02\n",
      "total_travel_time_min      6398.0  9.248560e+01     80.7518  0.000000e+00  4.000000e+01  8.400000e+01  1.250000e+02  1.020000e+03\n",
      "has_active_travel          6398.0  3.059000e-01      0.4608  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00\n",
      "has_transit_travel         6398.0  3.870000e-01      0.4871  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00\n",
      "has_hb_trip                6398.0  1.000000e+00      0.0000  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "age                        6398.0  3.541180e+01     15.4909 -7.000000e+00  2.600000e+01  3.200000e+01  4.400000e+01  9.900000e+01\n",
      "Female                     6398.0  5.414000e-01      0.4983  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "License                    6398.0  7.865000e-01      0.4098  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "Employed                   6398.0  7.330000e-01      0.4424  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "Student                    6398.0  2.008000e-01      0.4007  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "White                      6398.0  7.004000e-01      0.4581  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "Black                      6398.0  1.557000e-01      0.3626  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Asian                      6398.0  5.660000e-02      0.2311  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Other_Race                 6398.0  8.140000e-02      0.2735  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Hispanic                   6398.0  1.247000e-01      0.3304  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Bachelor_Above             6398.0  6.694000e-01      0.4705  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "Home_Owner                 6398.0  4.186000e-01      0.4934  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00\n",
      "Renter                     6398.0  5.503000e-01      0.4975  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "hhsize                     6398.0  2.553000e+00      1.4347  1.000000e+00  2.000000e+00  2.000000e+00  3.000000e+00  1.200000e+01\n",
      "hhveh                      6398.0  1.067700e+00      0.9203  0.000000e+00  0.000000e+00  1.000000e+00  2.000000e+00  8.000000e+00\n",
      "Zero_Vehicle_HH            6398.0  2.699000e-01      0.4440  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00\n",
      "hhinc                      6398.0  6.769600e+00      3.0859 -8.000000e+00  5.000000e+00  8.000000e+00  9.000000e+00  1.000000e+01\n",
      "Low_Income                 6398.0  7.660000e-02      0.2660  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Med_Income                 6398.0  9.210000e-02      0.2891  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "High_Income                6398.0  8.243000e-01      0.3806  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "dist_rail_ft               6398.0  4.101768e+03   4204.7032  1.849066e+02  1.536414e+03  2.655858e+03  4.816328e+03  3.238435e+04\n",
      "dist_bus_ft                6398.0  7.470601e+02    515.6113  8.501200e+00  3.938288e+02  6.465043e+02  1.091546e+03  5.223786e+03\n",
      "bus_count_14mile           6398.0  9.739400e+00      7.3168  0.000000e+00  4.000000e+00  8.000000e+00  1.400000e+01  3.500000e+01\n",
      "dist_rail_mi               6398.0  7.768000e-01      0.7963  3.500000e-02  2.910000e-01  5.030000e-01  9.122000e-01  6.133400e+00\n",
      "dist_bus_mi                6398.0  1.415000e-01      0.0977  1.600000e-03  7.460000e-02  1.224000e-01  2.067000e-01  9.894000e-01\n",
      "home_x                     6398.0  1.163645e+06  13895.9528  1.117267e+06  1.155921e+06  1.165348e+06  1.172287e+06  1.203415e+06\n",
      "home_y                     6398.0  1.906417e+06  29257.4170  1.816169e+06  1.891120e+06  1.914377e+06  1.928076e+06  1.951035e+06\n",
      "dist_cbd_ft                6398.0  2.433765e+04  16135.6945  0.000000e+00  1.214601e+04  2.305720e+04  3.506288e+04  7.911348e+04\n",
      "dist_cbd_mi                6398.0  4.609400e+00      3.0560  0.000000e+00  2.300400e+00  4.366900e+00  6.640700e+00  1.498360e+01\n",
      "inside_cbd                 6398.0  6.080000e-02      0.2390  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "H_intersection_density     6398.0  3.832021e+02    191.2503  0.000000e+00  2.860497e+02  3.832445e+02  4.873583e+02  1.189568e+03\n",
      "H_road_network_complexity  6398.0  2.885300e+00      0.4183  0.000000e+00  2.778400e+00  3.000000e+00  3.135900e+00  3.495100e+00\n",
      "H_building_density         6398.0  3.524000e-01      0.2996  0.000000e+00  2.558000e-01  3.375000e-01  3.909000e-01  2.708200e+00\n",
      "H_land_use_diversity       6398.0  1.022100e+00      0.5197 -0.000000e+00  6.931000e-01  1.098600e+00  1.418100e+00  1.979200e+00\n",
      "H_amenity_density          6398.0  2.636700e+00      2.9240  0.000000e+00  0.000000e+00  2.795800e+00  2.806200e+00  1.681490e+01\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      ">>> STEP 11: Saving Output\n",
      "--------------------------------------------------\n",
      "\n",
      "  ✓ Saved: ./city_homebased_chicago_research_ready_v2.csv\n",
      "  ✓ Records: 6,398\n",
      "  ✓ Variables: 51\n",
      "\n",
      "  VARIABLE LIST:\n",
      "  ----------------------------------------\n",
      "     1. sampno\n",
      "     2. perno\n",
      "     3. Y_total_MET_minutes\n",
      "     4. Y_transit_duration_min\n",
      "     5. Y_active_duration\n",
      "     6. n_walking_trips\n",
      "     7. n_cycling_trips\n",
      "     8. n_transit_trips\n",
      "     9. n_active_trips\n",
      "    10. n_hb_trips\n",
      "    11. n_total_trips\n",
      "    12. total_hb_travel_time_min\n",
      "    13. total_travel_time_min\n",
      "    14. has_active_travel\n",
      "    15. has_transit_travel\n",
      "    16. has_hb_trip\n",
      "    17. age\n",
      "    18. Female\n",
      "    19. License\n",
      "    20. Employed\n",
      "    21. Student\n",
      "    22. White\n",
      "    23. Black\n",
      "    24. Asian\n",
      "    25. Other_Race\n",
      "    26. Hispanic\n",
      "    27. Bachelor_Above\n",
      "    28. Home_Owner\n",
      "    29. Renter\n",
      "    30. hhsize\n",
      "    31. hhveh\n",
      "    32. Zero_Vehicle_HH\n",
      "    33. hhinc\n",
      "    34. Low_Income\n",
      "    35. Med_Income\n",
      "    36. High_Income\n",
      "    37. dist_rail_ft\n",
      "    38. dist_bus_ft\n",
      "    39. bus_count_14mile\n",
      "    40. dist_rail_mi\n",
      "    41. dist_bus_mi\n",
      "    42. home_x\n",
      "    43. home_y\n",
      "    44. dist_cbd_ft\n",
      "    45. dist_cbd_mi\n",
      "    46. inside_cbd\n",
      "    47. H_intersection_density\n",
      "    48. H_road_network_complexity\n",
      "    49. H_building_density\n",
      "    50. H_land_use_diversity\n",
      "    51. H_amenity_density\n",
      "\n",
      "======================================================================\n",
      "  PIPELINE COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "  Sample (first 3 rows, key columns):\n",
      "------------------------------------------------------------\n",
      "     sampno  perno  Y_active_duration  n_hb_trips   dist_cbd_ft  \\\n",
      "0  20000136      1                0.0           3      0.000000   \n",
      "1  20000136      2               24.0           3      0.000000   \n",
      "2  20000228      1                0.0           3  32934.071848   \n",
      "\n",
      "   H_intersection_density  \n",
      "0              391.833088  \n",
      "1              391.833088  \n",
      "2               64.485318  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        final_data = run_pipeline()\n",
    "        \n",
    "        print(\"\\n  Sample (first 3 rows, key columns):\")\n",
    "        print(\"-\" * 60)\n",
    "        key_cols = ['sampno', 'perno', 'Y_active_duration', 'n_hb_trips', 'dist_cbd_ft', 'H_intersection_density']\n",
    "        key_cols = [c for c in key_cols if c in final_data.columns]\n",
    "        print(final_data[key_cols].head(3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3527193-d0be-4956-a2f6-6c35459981bb",
   "metadata": {},
   "source": [
    "Combine with EPA data and Park Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05dc20e9-d232-4aa9-9541-4e43e88b6380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Chicago Travel Behavior - EPA SLD & Park Metrics Integration (FIXED)\n",
    "================================================================================\n",
    "FIXES:\n",
    "    1. Park data is POINTS - calculate distance only, no area calculation\n",
    "    2. EPA data matching issue - check GEOID format\n",
    "    3. Use Illinois state-level block group shapefile (tl_2025_17_bg.shp)\n",
    "\n",
    "NO missing value imputation - NaNs are preserved.\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794cb929-dc31-4c95-b959-6382b8f8be90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import KDTree\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b0c944-f102-4d60-b1e8-f399a63dfe02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"    EPA SLD & PARK METRICS INTEGRATION (FIXED)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExecution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"File paths and constants.\"\"\"\n",
    "    \n",
    "    # Input files\n",
    "    RESEARCH_READY_CSV = './city_homebased_chicago_research_ready_v2.csv'\n",
    "    \n",
    "    # FIXED: Use Illinois state-level block group shapefile\n",
    "    CBG_SHP = './tl_2025_17_bg/tl_2025_17_bg.shp'  # Illinois state block groups\n",
    "    \n",
    "    EPA_CSV = './epa_illinois_cleaned.csv'\n",
    "    PARK_SHP = './CPD_Facilities_20260123/geo_export_35b9829f-ebed-4a35-bea8-8f8412132942.shp'\n",
    "    \n",
    "    # Output file\n",
    "    OUTPUT_FILE = './city_home_based_chicago_research_ready_v3.1.csv'\n",
    "    \n",
    "    # Coordinate Reference Systems\n",
    "    CRS_WGS84 = 'EPSG:4326'\n",
    "    CRS_ILLINOIS = 'EPSG:3435'  # Illinois State Plane East (feet)\n",
    "    \n",
    "    # Distance constants\n",
    "    FEET_PER_MILE = 5280\n",
    "    \n",
    "    # EPA SLD columns to extract\n",
    "    EPA_COLUMNS = ['D1B', 'D1C', 'D2A_EPHHM']\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  → Input: {Config.RESEARCH_READY_CSV}\")\n",
    "print(f\"  → CBG Shapefile (IL state): {Config.CBG_SHP}\")\n",
    "print(f\"  → EPA CSV: {Config.EPA_CSV}\")\n",
    "print(f\"  → Park Shapefile: {Config.PARK_SHP}\")\n",
    "print(f\"  → Output: {Config.OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f761b90-f16f-494b-acc0-17470f547b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: LOAD BASE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 1] Loading Base Data / 加载基础数据\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load research-ready dataset\n",
    "df = pd.read_csv(Config.RESEARCH_READY_CSV)\n",
    "print(f\"\\n  ✓ Loaded research-ready data: {len(df):,} observations\")\n",
    "\n",
    "# Create GeoDataFrame from home locations\n",
    "print(\"\\n  Creating GeoDataFrame from home locations...\")\n",
    "\n",
    "if 'home_x' in df.columns and 'home_y' in df.columns:\n",
    "    # Use existing projected coordinates (EPSG:3435)\n",
    "    valid_mask = df['home_x'].notna() & df['home_y'].notna() & (df['home_x'] != 0) & (df['home_y'] != 0)\n",
    "    df_valid = df[valid_mask].copy()\n",
    "    \n",
    "    geometry = [Point(x, y) for x, y in zip(df_valid['home_x'], df_valid['home_y'])]\n",
    "    gdf = gpd.GeoDataFrame(df_valid, geometry=geometry, crs=Config.CRS_ILLINOIS)\n",
    "    \n",
    "    print(f\"  ✓ Created GeoDataFrame: {len(gdf):,} valid points\")\n",
    "    print(f\"    CRS: {Config.CRS_ILLINOIS}\")\n",
    "else:\n",
    "    raise ValueError(\"home_x/home_y columns not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b5ba4a-f9bf-43be-a69b-4944c0ba4cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: EPA SMART LOCATION DATABASE INTEGRATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 2] EPA Smart Location Database Integration / EPA智能位置数据库整合\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Step 2.1: Load Illinois State Block Group shapefile\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n  [2.1] Loading Illinois State Block Group shapefile...\")\n",
    "\n",
    "if not os.path.exists(Config.CBG_SHP):\n",
    "    print(f\"    ⚠ CBG shapefile not found: {Config.CBG_SHP}\")\n",
    "    cbg_gdf = None\n",
    "else:\n",
    "    cbg_gdf = gpd.read_file(Config.CBG_SHP)\n",
    "    print(f\"    ✓ Loaded: {len(cbg_gdf):,} block groups (entire Illinois)\")\n",
    "    print(f\"    Columns: {cbg_gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Find GEOID column\n",
    "    geoid_col = None\n",
    "    for col in ['GEOID', 'GEOID20', 'GEOID10', 'geoid']:\n",
    "        if col in cbg_gdf.columns:\n",
    "            geoid_col = col\n",
    "            break\n",
    "    \n",
    "    if geoid_col:\n",
    "        print(f\"    ✓ Found GEOID column: {geoid_col}\")\n",
    "        print(f\"    Sample GEOID: {cbg_gdf[geoid_col].iloc[0]}\")\n",
    "        print(f\"    GEOID length: {len(str(cbg_gdf[geoid_col].iloc[0]))}\")\n",
    "    else:\n",
    "        print(f\"    ⚠ No GEOID column found!\")\n",
    "        print(f\"    Available columns: {cbg_gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Transform to EPSG:3435\n",
    "    if cbg_gdf.crs is None:\n",
    "        cbg_gdf = cbg_gdf.set_crs(Config.CRS_WGS84)\n",
    "    cbg_gdf = cbg_gdf.to_crs(Config.CRS_ILLINOIS)\n",
    "    print(f\"    ✓ Projected to: {Config.CRS_ILLINOIS}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Step 2.2: Load EPA Smart Location Database CSV\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n  [2.2] Loading EPA Smart Location Database CSV...\")\n",
    "\n",
    "if not os.path.exists(Config.EPA_CSV):\n",
    "    print(f\"    ⚠ EPA CSV not found: {Config.EPA_CSV}\")\n",
    "    epa_df = None\n",
    "else:\n",
    "    epa_df = pd.read_csv(Config.EPA_CSV, low_memory=False)\n",
    "    print(f\"    ✓ Loaded: {len(epa_df):,} records\")\n",
    "    \n",
    "    # Find GEOID column in EPA data\n",
    "    epa_geoid_col = None\n",
    "    for col in ['GEOID20', 'GEOID10', 'GEOID', 'geoid20', 'geoid', 'BGFIPS']:\n",
    "        if col in epa_df.columns:\n",
    "            epa_geoid_col = col\n",
    "            break\n",
    "    \n",
    "    if epa_geoid_col:\n",
    "        print(f\"    ✓ Found EPA GEOID column: {epa_geoid_col}\")\n",
    "        sample_geoid = str(epa_df[epa_geoid_col].iloc[0])\n",
    "        print(f\"    Sample GEOID: {sample_geoid}\")\n",
    "        print(f\"    GEOID length: {len(sample_geoid)}\")\n",
    "    else:\n",
    "        print(f\"    ⚠ No GEOID column found in EPA data!\")\n",
    "        print(f\"    Columns: {epa_df.columns.tolist()[:30]}\")\n",
    "    \n",
    "    # Check for required EPA columns\n",
    "    available_epa = [col for col in Config.EPA_COLUMNS if col in epa_df.columns]\n",
    "    missing_epa = [col for col in Config.EPA_COLUMNS if col not in epa_df.columns]\n",
    "    print(f\"    ✓ Available EPA columns: {available_epa}\")\n",
    "    if missing_epa:\n",
    "        print(f\"    ⚠ Missing EPA columns: {missing_epa}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Step 2.3: Spatial Join - Assign GEOID to each household\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n  [2.3] Performing Spatial Join (households → block groups)...\")\n",
    "\n",
    "if cbg_gdf is not None and geoid_col is not None:\n",
    "    # Perform spatial join\n",
    "    gdf_with_geoid = gpd.sjoin(\n",
    "        gdf,\n",
    "        cbg_gdf[[geoid_col, 'geometry']],\n",
    "        how='left',\n",
    "        predicate='within'\n",
    "    )\n",
    "    \n",
    "    # Rename GEOID column\n",
    "    gdf_with_geoid = gdf_with_geoid.rename(columns={geoid_col: 'GEOID_CBG'})\n",
    "    \n",
    "    # Drop duplicate index column\n",
    "    if 'index_right' in gdf_with_geoid.columns:\n",
    "        gdf_with_geoid = gdf_with_geoid.drop(columns=['index_right'])\n",
    "    \n",
    "    # Count matches\n",
    "    matched = gdf_with_geoid['GEOID_CBG'].notna().sum()\n",
    "    unmatched = gdf_with_geoid['GEOID_CBG'].isna().sum()\n",
    "    \n",
    "    print(f\"    ✓ Spatial join complete\")\n",
    "    print(f\"      Matched: {matched:,} ({matched/len(gdf_with_geoid)*100:.1f}%)\")\n",
    "    print(f\"      Unmatched: {unmatched:,} ({unmatched/len(gdf_with_geoid)*100:.1f}%)\")\n",
    "    \n",
    "    # Show sample matched GEOIDs\n",
    "    sample_matched = gdf_with_geoid['GEOID_CBG'].dropna().head(3).tolist()\n",
    "    print(f\"      Sample matched GEOIDs: {sample_matched}\")\n",
    "    \n",
    "else:\n",
    "    print(\"    ⚠ Skipping spatial join (missing CBG data)\")\n",
    "    gdf_with_geoid = gdf.copy()\n",
    "    gdf_with_geoid['GEOID_CBG'] = np.nan\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Step 2.4: Format GEOIDs for matching\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n  [2.4] Formatting GEOIDs for matching...\")\n",
    "\n",
    "if 'GEOID_CBG' in gdf_with_geoid.columns and epa_df is not None and epa_geoid_col is not None:\n",
    "    \n",
    "    # Check GEOID formats before conversion\n",
    "    sample_cbg_geoid = gdf_with_geoid['GEOID_CBG'].dropna().iloc[0] if gdf_with_geoid['GEOID_CBG'].notna().any() else None\n",
    "    sample_epa_geoid = epa_df[epa_geoid_col].iloc[0]\n",
    "    \n",
    "    print(f\"    CBG GEOID sample (before): {sample_cbg_geoid} (type: {type(sample_cbg_geoid).__name__})\")\n",
    "    print(f\"    EPA GEOID sample (before): {sample_epa_geoid} (type: {type(sample_epa_geoid).__name__})\")\n",
    "    \n",
    "    # Determine target length (use EPA as reference)\n",
    "    epa_geoid_length = len(str(sample_epa_geoid).replace('.0', ''))\n",
    "    print(f\"    Target GEOID length: {epa_geoid_length}\")\n",
    "    \n",
    "    # Format CBG GEOID\n",
    "    gdf_with_geoid['GEOID_CBG_STR'] = (\n",
    "        gdf_with_geoid['GEOID_CBG']\n",
    "        .astype(str)\n",
    "        .str.replace('.0', '', regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    \n",
    "    # Handle NaN values\n",
    "    gdf_with_geoid.loc[\n",
    "        gdf_with_geoid['GEOID_CBG_STR'].isin(['nan', 'None', '']), \n",
    "        'GEOID_CBG_STR'\n",
    "    ] = np.nan\n",
    "    \n",
    "    # Zero-pad if needed\n",
    "    gdf_with_geoid['GEOID_CBG_STR'] = (\n",
    "        gdf_with_geoid['GEOID_CBG_STR']\n",
    "        .str.zfill(epa_geoid_length)\n",
    "    )\n",
    "    \n",
    "    # Format EPA GEOID\n",
    "    epa_df['GEOID_EPA_STR'] = (\n",
    "        epa_df[epa_geoid_col]\n",
    "        .astype(str)\n",
    "        .str.replace('.0', '', regex=False)\n",
    "        .str.strip()\n",
    "        .str.zfill(epa_geoid_length)\n",
    "    )\n",
    "    \n",
    "    # Check formatted samples\n",
    "    sample_cbg_formatted = gdf_with_geoid['GEOID_CBG_STR'].dropna().iloc[0] if gdf_with_geoid['GEOID_CBG_STR'].notna().any() else None\n",
    "    sample_epa_formatted = epa_df['GEOID_EPA_STR'].iloc[0]\n",
    "    \n",
    "    print(f\"    CBG GEOID sample (after): {sample_cbg_formatted}\")\n",
    "    print(f\"    EPA GEOID sample (after): {sample_epa_formatted}\")\n",
    "    \n",
    "    # Check overlap\n",
    "    cbg_geoids = set(gdf_with_geoid['GEOID_CBG_STR'].dropna().unique())\n",
    "    epa_geoids = set(epa_df['GEOID_EPA_STR'].unique())\n",
    "    common_geoids = cbg_geoids & epa_geoids\n",
    "    \n",
    "    print(f\"    CBG unique GEOIDs: {len(cbg_geoids):,}\")\n",
    "    print(f\"    EPA unique GEOIDs: {len(epa_geoids):,}\")\n",
    "    print(f\"    Common GEOIDs: {len(common_geoids):,}\")\n",
    "    \n",
    "    if len(common_geoids) == 0:\n",
    "        print(\"    ⚠ WARNING: No common GEOIDs found!\")\n",
    "        print(f\"    CBG GEOID samples: {list(cbg_geoids)[:5]}\")\n",
    "        print(f\"    EPA GEOID samples: {list(epa_geoids)[:5]}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Step 2.5: Attribute Merge - Attach EPA metrics\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n  [2.5] Merging EPA SLD attributes...\")\n",
    "\n",
    "if epa_df is not None and 'GEOID_CBG_STR' in gdf_with_geoid.columns:\n",
    "    # Select needed columns\n",
    "    epa_cols_to_merge = ['GEOID_EPA_STR'] + [c for c in Config.EPA_COLUMNS if c in epa_df.columns]\n",
    "    epa_subset = epa_df[epa_cols_to_merge].copy()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    epa_subset = epa_subset.drop_duplicates(subset=['GEOID_EPA_STR'], keep='first')\n",
    "    print(f\"    EPA records for merge: {len(epa_subset):,}\")\n",
    "    \n",
    "    # Perform merge\n",
    "    gdf_with_epa = gdf_with_geoid.merge(\n",
    "        epa_subset,\n",
    "        left_on='GEOID_CBG_STR',\n",
    "        right_on='GEOID_EPA_STR',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    cols_to_drop = ['GEOID_EPA_STR', 'GEOID_CBG_STR']\n",
    "    gdf_with_epa = gdf_with_epa.drop(columns=[c for c in cols_to_drop if c in gdf_with_epa.columns])\n",
    "    \n",
    "    # Report match statistics\n",
    "    print(\"\\n    EPA merge results:\")\n",
    "    for col in Config.EPA_COLUMNS:\n",
    "        if col in gdf_with_epa.columns:\n",
    "            matched = gdf_with_epa[col].notna().sum()\n",
    "            pct = matched / len(gdf_with_epa) * 100\n",
    "            print(f\"      {col}: {matched:,} matched ({pct:.1f}%)\")\n",
    "        else:\n",
    "            gdf_with_epa[col] = np.nan\n",
    "            print(f\"      {col}: NOT FOUND\")\n",
    "\n",
    "else:\n",
    "    print(\"    ⚠ Skipping EPA merge\")\n",
    "    gdf_with_epa = gdf_with_geoid.copy()\n",
    "    for col in Config.EPA_COLUMNS:\n",
    "        gdf_with_epa[col] = np.nan\n",
    "\n",
    "print(f\"\\n  ✓ EPA integration complete: {len(gdf_with_epa):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c3e1e3-d3c8-4d1e-9ed0-74eb6f20bb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: PARK METRICS - DISTANCE ONLY (Parks are POINTS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 3] Park Distance Calculation / 公园距离计算\")\n",
    "print(\"=\" * 80)\n",
    "print(\"  Note: Park data contains POINTS only - calculating distance, no area\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Step 3.1: Load Park shapefile\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n  [3.1] Loading Park shapefile...\")\n",
    "\n",
    "if not os.path.exists(Config.PARK_SHP):\n",
    "    print(f\"    ⚠ Park shapefile not found: {Config.PARK_SHP}\")\n",
    "    parks_gdf = None\n",
    "else:\n",
    "    parks_gdf = gpd.read_file(Config.PARK_SHP)\n",
    "    print(f\"    ✓ Loaded: {len(parks_gdf):,} park features\")\n",
    "    print(f\"    Columns: {parks_gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Check geometry types\n",
    "    geom_types = parks_gdf.geometry.geom_type.value_counts()\n",
    "    print(f\"    Geometry types: {geom_types.to_dict()}\")\n",
    "    \n",
    "    # Confirm it's points\n",
    "    if 'Point' in geom_types.index:\n",
    "        print(\"    ✓ Confirmed: Park data contains POINT geometries\")\n",
    "    \n",
    "    # Transform to EPSG:3435\n",
    "    if parks_gdf.crs is None:\n",
    "        parks_gdf = parks_gdf.set_crs(Config.CRS_WGS84)\n",
    "    parks_gdf = parks_gdf.to_crs(Config.CRS_ILLINOIS)\n",
    "    print(f\"    ✓ Projected to: {Config.CRS_ILLINOIS}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Step 3.2: Calculate Distance to Nearest Park using KDTree\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n  [3.2] Calculating distance to nearest park (KDTree)...\")\n",
    "\n",
    "if parks_gdf is not None and len(parks_gdf) > 0:\n",
    "    \n",
    "    # Extract park coordinates\n",
    "    park_coords = np.column_stack([\n",
    "        parks_gdf.geometry.x.values,\n",
    "        parks_gdf.geometry.y.values\n",
    "    ])\n",
    "    \n",
    "    # Build KDTree\n",
    "    park_tree = KDTree(park_coords)\n",
    "    print(f\"    ✓ Built KDTree from {len(park_coords):,} park points\")\n",
    "    \n",
    "    # Extract home coordinates\n",
    "    home_coords = np.column_stack([\n",
    "        gdf_with_epa.geometry.x.values,\n",
    "        gdf_with_epa.geometry.y.values\n",
    "    ])\n",
    "    \n",
    "    # Query nearest park for each home\n",
    "    distances_ft, indices = park_tree.query(home_coords)\n",
    "    \n",
    "    # Add to dataframe\n",
    "    gdf_with_epa['dist_park_ft'] = distances_ft\n",
    "    gdf_with_epa['dist_park_mi'] = distances_ft / Config.FEET_PER_MILE\n",
    "    \n",
    "    # Report statistics\n",
    "    print(f\"    ✓ Distance calculation complete\")\n",
    "    print(f\"      Mean distance: {gdf_with_epa['dist_park_mi'].mean():.2f} miles\")\n",
    "    print(f\"      Median distance: {gdf_with_epa['dist_park_mi'].median():.2f} miles\")\n",
    "    print(f\"      Min: {gdf_with_epa['dist_park_mi'].min():.3f} miles\")\n",
    "    print(f\"      Max: {gdf_with_epa['dist_park_mi'].max():.2f} miles\")\n",
    "    \n",
    "else:\n",
    "    print(\"    ⚠ Skipping park distance (no park data)\")\n",
    "    gdf_with_epa['dist_park_ft'] = np.nan\n",
    "    gdf_with_epa['dist_park_mi'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe09f8a-b425-45ab-aa2e-8454d35cc744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: FINALIZE AND MERGE BACK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 4] Finalizing Dataset / 最终数据整合\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# New columns to add\n",
    "new_columns = ['GEOID_CBG', 'D1B', 'D1C', 'D2A_EPHHM', 'dist_park_mi']\n",
    "\n",
    "# Select columns for merge\n",
    "merge_cols = ['sampno', 'perno'] + [c for c in new_columns if c in gdf_with_epa.columns]\n",
    "new_data = gdf_with_epa[merge_cols].copy()\n",
    "\n",
    "# Remove duplicates\n",
    "new_data = new_data.drop_duplicates(subset=['sampno', 'perno'], keep='first')\n",
    "print(f\"\\n  Prepared {len(new_data):,} records with new columns\")\n",
    "\n",
    "# Convert join keys to same type\n",
    "df['sampno'] = pd.to_numeric(df['sampno'], errors='coerce').astype('Int64')\n",
    "df['perno'] = pd.to_numeric(df['perno'], errors='coerce').astype('Int64')\n",
    "new_data['sampno'] = pd.to_numeric(new_data['sampno'], errors='coerce').astype('Int64')\n",
    "new_data['perno'] = pd.to_numeric(new_data['perno'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Left merge\n",
    "final_df = df.merge(\n",
    "    new_data,\n",
    "    on=['sampno', 'perno'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n  ✓ Final dataset: {len(final_df):,} rows\")\n",
    "print(f\"    Original rows: {len(df):,}\")\n",
    "print(f\"    Row preservation: {'✓ OK' if len(final_df) == len(df) else '✗ WARNING'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beaf2ac5-727c-41dc-9819-547c49aec097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: MISSING VALUE REPORT (NO IMPUTATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 5] Missing Value Report / 缺失值报告\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n  *** NO IMPUTATION PERFORMED - NaNs PRESERVED ***\")\n",
    "print(\"\\n  Missing value counts for new columns:\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "\n",
    "for col in new_columns:\n",
    "    if col in final_df.columns:\n",
    "        na_count = final_df[col].isna().sum()\n",
    "        na_pct = na_count / len(final_df) * 100\n",
    "        valid_count = final_df[col].notna().sum()\n",
    "        print(f\"    {col}:\")\n",
    "        print(f\"      Missing (NaN): {na_count:,} ({na_pct:.1f}%)\")\n",
    "        print(f\"      Valid: {valid_count:,} ({100-na_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577fcc21-d7ec-4967-8347-9e0f4dda2a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "    EPA SLD & PARK METRICS INTEGRATION (FIXED)\n",
      "    EPA智能位置数据库 & 公园指标整合（修复版）\n",
      "================================================================================\n",
      "\n",
      "Execution Time: 2026-02-10 20:54:33\n",
      "\n",
      "Configuration:\n",
      "  → Input: ./city_homebased_chicago_research_ready_v2.csv\n",
      "  → CBG Shapefile (IL state): ./tl_2025_17_bg/tl_2025_17_bg.shp\n",
      "  → EPA CSV: ./epa_illinois_cleaned.csv\n",
      "  → Park Shapefile: ./CPD_Facilities_20260123/geo_export_35b9829f-ebed-4a35-bea8-8f8412132942.shp\n",
      "  → Output: ./city_home_based_chicago_research_ready_v3.1.csv\n",
      "\n",
      "================================================================================\n",
      "[STEP 1] Loading Base Data / 加载基础数据\n",
      "================================================================================\n",
      "\n",
      "  ✓ Loaded research-ready data: 6,398 observations\n",
      "\n",
      "  Creating GeoDataFrame from home locations...\n",
      "  ✓ Created GeoDataFrame: 6,398 valid points\n",
      "    CRS: EPSG:3435\n",
      "\n",
      "================================================================================\n",
      "[STEP 2] EPA Smart Location Database Integration / EPA智能位置数据库整合\n",
      "================================================================================\n",
      "\n",
      "  [2.1] Loading Illinois State Block Group shapefile...\n",
      "    ✓ Loaded: 9,898 block groups (entire Illinois)\n",
      "    Columns: ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'GEOIDFQ', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON', 'geometry']\n",
      "    ✓ Found GEOID column: GEOID\n",
      "    Sample GEOID: 172010038102\n",
      "    GEOID length: 12\n",
      "    ✓ Projected to: EPSG:3435\n",
      "\n",
      "  [2.2] Loading EPA Smart Location Database CSV...\n",
      "    ✓ Loaded: 9,691 records\n",
      "    ✓ Found EPA GEOID column: GEOID\n",
      "    Sample GEOID: 171950012004\n",
      "    GEOID length: 12\n",
      "    ✓ Available EPA columns: ['D1B', 'D1C', 'D2A_EPHHM']\n",
      "\n",
      "  [2.3] Performing Spatial Join (households → block groups)...\n",
      "    ✓ Spatial join complete\n",
      "      Matched: 6,398 (100.0%)\n",
      "      Unmatched: 0 (0.0%)\n",
      "      Sample matched GEOIDs: ['170310814021', '170310814021', '170318425001']\n",
      "\n",
      "  [2.4] Formatting GEOIDs for matching...\n",
      "    CBG GEOID sample (before): 170310814021 (type: str)\n",
      "    EPA GEOID sample (before): 171950012004 (type: int64)\n",
      "    Target GEOID length: 12\n",
      "    CBG GEOID sample (after): 170310814021\n",
      "    EPA GEOID sample (after): 171950012004\n",
      "    CBG unique GEOIDs: 673\n",
      "    EPA unique GEOIDs: 9,691\n",
      "    Common GEOIDs: 662\n",
      "\n",
      "  [2.5] Merging EPA SLD attributes...\n",
      "    EPA records for merge: 9,691\n",
      "\n",
      "    EPA merge results:\n",
      "      D1B: 6,179 matched (96.6%)\n",
      "      D1C: 6,179 matched (96.6%)\n",
      "      D2A_EPHHM: 6,179 matched (96.6%)\n",
      "\n",
      "  ✓ EPA integration complete: 6,398 records\n",
      "\n",
      "================================================================================\n",
      "[STEP 3] Park Distance Calculation / 公园距离计算\n",
      "================================================================================\n",
      "  Note: Park data contains POINTS only - calculating distance, no area\n",
      "\n",
      "  [3.1] Loading Park shapefile...\n",
      "    ✓ Loaded: 4,467 park features\n",
      "    Columns: ['objectid_1', 'park', 'park_no', 'facility_n', 'facility_t', 'x_coord', 'y_coord', 'gisobjid', 'geometry']\n",
      "    Geometry types: {'Point': 4467}\n",
      "    ✓ Confirmed: Park data contains POINT geometries\n",
      "    ✓ Projected to: EPSG:3435\n",
      "\n",
      "  [3.2] Calculating distance to nearest park (KDTree)...\n",
      "    ✓ Built KDTree from 4,467 park points\n",
      "    ✓ Distance calculation complete\n",
      "      Mean distance: 0.21 miles\n",
      "      Median distance: 0.19 miles\n",
      "      Min: 0.007 miles\n",
      "      Max: 1.14 miles\n",
      "\n",
      "================================================================================\n",
      "[STEP 4] Finalizing Dataset / 最终数据整合\n",
      "================================================================================\n",
      "\n",
      "  Prepared 6,398 records with new columns\n",
      "\n",
      "  ✓ Final dataset: 6,398 rows\n",
      "    Original rows: 6,398\n",
      "    Row preservation: ✓ OK\n",
      "\n",
      "================================================================================\n",
      "[STEP 5] Missing Value Report / 缺失值报告\n",
      "================================================================================\n",
      "\n",
      "  *** NO IMPUTATION PERFORMED - NaNs PRESERVED ***\n",
      "\n",
      "  Missing value counts for new columns:\n",
      "  --------------------------------------------------\n",
      "    GEOID_CBG:\n",
      "      Missing (NaN): 0 (0.0%)\n",
      "      Valid: 6,398 (100.0%)\n",
      "    D1B:\n",
      "      Missing (NaN): 219 (3.4%)\n",
      "      Valid: 6,179 (96.6%)\n",
      "    D1C:\n",
      "      Missing (NaN): 219 (3.4%)\n",
      "      Valid: 6,179 (96.6%)\n",
      "    D2A_EPHHM:\n",
      "      Missing (NaN): 219 (3.4%)\n",
      "      Valid: 6,179 (96.6%)\n",
      "    dist_park_mi:\n",
      "      Missing (NaN): 0 (0.0%)\n",
      "      Valid: 6,398 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "[STEP 6] Saving Output / 保存输出\n",
      "================================================================================\n",
      "\n",
      "  ✓ Saved: ./city_home_based_chicago_research_ready_v3.1.csv\n",
      "    Rows: 6,398\n",
      "    Columns: 56\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY / 最终总结\n",
      "================================================================================\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║           EPA SLD & PARK METRICS INTEGRATION COMPLETE (FIXED)                ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "  Fixes Applied / 已修复问题:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "    1. ✓ Park data is POINTS - distance only, no area calculation\n",
      "    2. ✓ Using Illinois state block group shapefile for better coverage\n",
      "    3. ✓ Improved GEOID format matching\n",
      "\n",
      "  New Variables Added / 新增变量:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "    • GEOID_CBG: 6,398 valid | 0 missing\n",
      "    • D1B: 6,179 valid | 219 missing\n",
      "    • D1C: 6,179 valid | 219 missing\n",
      "    • D2A_EPHHM: 6,179 valid | 219 missing\n",
      "    • dist_park_mi: 6,398 valid | 0 missing\n",
      "\n",
      "  Note / 注意:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "    ⚠ NO IMPUTATION - All NaN values preserved\n",
      "    ⚠ park_acres_half_mile NOT calculated (parks are points, not polygons)\n",
      "\n",
      "  Output / 输出:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "    ./city_home_based_chicago_research_ready_v3.1.csv\n",
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: SAVE OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 6] Saving Output / 保存输出\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_df.to_csv(Config.OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\n  ✓ Saved: {Config.OUTPUT_FILE}\")\n",
    "print(f\"    Rows: {len(final_df):,}\")\n",
    "print(f\"    Columns: {len(final_df.columns)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY / 最终总结\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║           EPA SLD & PARK METRICS INTEGRATION COMPLETE (FIXED)                ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "  Fixes Applied / 已修复问题:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "    1. ✓ Park data is POINTS - distance only, no area calculation\n",
    "    2. ✓ Using Illinois state block group shapefile for better coverage\n",
    "    3. ✓ Improved GEOID format matching\n",
    "\n",
    "  New Variables Added / 新增变量:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "for col in new_columns:\n",
    "    if col in final_df.columns:\n",
    "        na_count = final_df[col].isna().sum()\n",
    "        valid_count = final_df[col].notna().sum()\n",
    "        print(f\"    • {col}: {valid_count:,} valid | {na_count:,} missing\")\n",
    "\n",
    "print(f\"\"\"\n",
    "  Note / 注意:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "    ⚠ NO IMPUTATION - All NaN values preserved\n",
    "    ⚠ park_acres_half_mile NOT calculated (parks are points, not polygons)\n",
    "\n",
    "  Output / 输出:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "    {Config.OUTPUT_FILE}\n",
    "\n",
    "══════════════════════════════════════════════════════════════════════════════\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed084c3-6a3c-45df-9b4a-14d0b7dab6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Remove Missing and Negative Values - Create Clean Dataset\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5781151b-dc8f-4eed-8da6-9c782db8397c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "    REMOVE MISSING & NEGATIVE VALUES\n",
      "================================================================================\n",
      "\n",
      "Original data: 6,398 observations, 56 variables\n",
      "\n",
      "================================================================================\n",
      "[STEP 1] Identifying Negative Values / 识别负值\n",
      "================================================================================\n",
      "\n",
      "  Variables with negative values (before cleaning):\n",
      "  ------------------------------------------------------------\n",
      "    age                                 6 negatives (min: -7)\n",
      "    hhinc                              45 negatives (min: -8)\n",
      "\n",
      "  Total variables with negatives: 2\n",
      "\n",
      "================================================================================\n",
      "[STEP 2] Replacing Negative Values with NaN / 将负值替换为NaN\n",
      "================================================================================\n",
      "    age: 6 negative values → NaN\n",
      "    hhinc: 45 negative values → NaN\n",
      "\n",
      "================================================================================\n",
      "[STEP 3] Missing Values Summary (After Negative Removal)\n",
      "================================================================================\n",
      "\n",
      "  Variable                        N_Missing  % Missing\n",
      "  -------------------------------------------------------\n",
      "  D1B                                   219      3.42%\n",
      "  D1C                                   219      3.42%\n",
      "  D2A_EPHHM                             219      3.42%\n",
      "  hhinc                                  45      0.70%\n",
      "  age                                     6      0.09%\n",
      "\n",
      "================================================================================\n",
      "[STEP 4] Removing Rows with Missing Values / 删除含缺失值的行\n",
      "================================================================================\n",
      "\n",
      "  Before: 6,398 observations\n",
      "  After:  6,129 observations\n",
      "  Removed: 269 observations (4.2%)\n",
      "\n",
      "================================================================================\n",
      "[STEP 5] Verification / 验证\n",
      "================================================================================\n",
      "\n",
      "  Remaining missing values: 0\n",
      "  Remaining negative values: 0\n",
      "\n",
      "  ✓ Dataset is clean - no missing or negative values!\n",
      "\n",
      "================================================================================\n",
      "[STEP 6] Summary Statistics (Clean Data) / 清洗后统计\n",
      "================================================================================\n",
      "\n",
      "Variable                             N         Mean          Std        Min          Max\n",
      "==========================================================================================\n",
      "Y_total_MET_minutes              6,129      42.5681     117.1740     0.0000    3102.0000\n",
      "Y_transit_duration_min           6,129      31.7696      56.6117     0.0000     960.0000\n",
      "Y_active_duration                6,129      10.3389      26.7428     0.0000     629.0000\n",
      "n_walking_trips                  6,129       0.5078       1.0179     0.0000      11.0000\n",
      "n_cycling_trips                  6,129       0.1274       0.5371     0.0000       6.0000\n",
      "n_transit_trips                  6,129       0.6815       0.9467     0.0000       8.0000\n",
      "n_active_trips                   6,129       0.6352       1.1392     0.0000      11.0000\n",
      "n_hb_trips                       6,129       3.2783       1.4772     1.0000      13.0000\n",
      "n_total_trips                    6,129       4.4546       2.4001     1.0000      19.0000\n",
      "total_hb_travel_time_min         6,129      66.7580      63.5173     0.0000     965.0000\n",
      "total_travel_time_min            6,129      92.8507      80.8316     0.0000    1020.0000\n",
      "has_active_travel                6,129       0.3043       0.4601     0.0000       1.0000\n",
      "has_transit_travel               6,129       0.3903       0.4879     0.0000       1.0000\n",
      "has_hb_trip                      6,129       1.0000       0.0000     1.0000       1.0000\n",
      "age                              6,129      35.2721      15.3770     5.0000      99.0000\n",
      "Female                           6,129       0.5415       0.4983     0.0000       1.0000\n",
      "License                          6,129       0.7830       0.4122     0.0000       1.0000\n",
      "Employed                         6,129       0.7319       0.4430     0.0000       1.0000\n",
      "Student                          6,129       0.2030       0.4022     0.0000       1.0000\n",
      "White                            6,129       0.7042       0.4564     0.0000       1.0000\n",
      "Black                            6,129       0.1557       0.3626     0.0000       1.0000\n",
      "Asian                            6,129       0.0545       0.2270     0.0000       1.0000\n",
      "Other_Race                       6,129       0.0804       0.2720     0.0000       1.0000\n",
      "Hispanic                         6,129       0.1260       0.3318     0.0000       1.0000\n",
      "Bachelor_Above                   6,129       0.6650       0.4720     0.0000       1.0000\n",
      "Home_Owner                       6,129       0.4167       0.4931     0.0000       1.0000\n",
      "Renter                           6,129       0.5516       0.4974     0.0000       1.0000\n",
      "hhsize                           6,129       2.5680       1.4479     1.0000      12.0000\n",
      "hhveh                            6,129       1.0689       0.9255     0.0000       8.0000\n",
      "Zero_Vehicle_HH                  6,129       0.2715       0.4448     0.0000       1.0000\n",
      "hhinc                            6,129       6.8411       2.8676     1.0000      10.0000\n",
      "Low_Income                       6,129       0.0783       0.2687     0.0000       1.0000\n",
      "Med_Income                       6,129       0.0940       0.2918     0.0000       1.0000\n",
      "High_Income                      6,129       0.8277       0.3777     0.0000       1.0000\n",
      "dist_rail_ft                     6,129    4117.5802    4220.8791   184.9066   32384.3531\n",
      "dist_bus_ft                      6,129     728.0757     496.6031     8.5012    5223.7856\n",
      "bus_count_14mile                 6,129       9.8784       7.2824     0.0000      35.0000\n",
      "dist_rail_mi                     6,129       0.7798       0.7994     0.0350       6.1334\n",
      "dist_bus_mi                      6,129       0.1379       0.0941     0.0016       0.9894\n",
      "home_x                           6,129 1163141.4777   13852.1939 1117266.9331 1203414.9275\n",
      "home_y                           6,129 1906845.6854   29401.0241 1816168.7690 1951034.5660\n",
      "dist_cbd_ft                      6,129   24854.4690   15879.8661     0.0000   79113.4767\n",
      "dist_cbd_mi                      6,129       4.7073       3.0076     0.0000      14.9836\n",
      "inside_cbd                       6,129       0.0519       0.2218     0.0000       1.0000\n",
      "H_intersection_density           6,129     378.3024     181.9969     0.0000    1080.2722\n",
      "H_road_network_complexity        6,129       2.8940       0.4162     0.0000       3.4951\n",
      "H_building_density               6,129       0.3173       0.1375     0.0000       2.7082\n",
      "H_land_use_diversity             6,129       1.0224       0.5245    -0.0000       1.9792\n",
      "H_amenity_density                6,129       2.6790       2.9379     0.0000      16.8149\n",
      "D1B                              6,129      40.7129      38.5440     0.0000     473.2440\n",
      "D1C                              6,129      20.4526     106.7384     0.0000    1391.6122\n",
      "D2A_EPHHM                        6,129       0.4439       0.2204     0.0000       0.9842\n",
      "dist_park_mi                     6,129       0.2131       0.1415     0.0073       1.1416\n",
      "\n",
      "================================================================================\n",
      "[STEP 7] Save Clean Dataset / 保存清洗后数据\n",
      "================================================================================\n",
      "\n",
      "  ✓ Saved: ./city_home_based_chicago_research_ready_v3_clean.csv\n",
      "    Rows: 6,129\n",
      "    Columns: 56\n",
      "  ✓ Saved: ./city_home_based_summary_statistics_clean.csv\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY / 最终总结\n",
      "================================================================================\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                    CLEAN DATASET CREATED                                     ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "  Data Cleaning Summary / 数据清洗总结:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "    Original observations:        6,398\n",
      "    Clean observations:           6,129\n",
      "    Removed observations:         269 (4.2%)\n",
      "\n",
      "  Cleaning Steps / 清洗步骤:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "    1. Replaced negative values (survey codes -7, -8, -9, etc.) with NaN\n",
      "    2. Removed all rows containing any NaN values\n",
      "\n",
      "  Verification / 验证:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "    Remaining missing values:     0\n",
      "    Remaining negative values:    0\n",
      "\n",
      "  Output Files / 输出文件:\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "    ✓ ./chicago_research_ready_v3_clean.csv   (clean dataset)\n",
      "    ✓ ./summary_statistics_clean.csv          (summary stats)\n",
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "\n",
      "  Variables in clean dataset:\n",
      "  --------------------------------------------------\n",
      "     1. sampno\n",
      "     2. perno\n",
      "     3. Y_total_MET_minutes\n",
      "     4. Y_transit_duration_min\n",
      "     5. Y_active_duration\n",
      "     6. n_walking_trips\n",
      "     7. n_cycling_trips\n",
      "     8. n_transit_trips\n",
      "     9. n_active_trips\n",
      "    10. n_hb_trips\n",
      "    11. n_total_trips\n",
      "    12. total_hb_travel_time_min\n",
      "    13. total_travel_time_min\n",
      "    14. has_active_travel\n",
      "    15. has_transit_travel\n",
      "    16. has_hb_trip\n",
      "    17. age\n",
      "    18. Female\n",
      "    19. License\n",
      "    20. Employed\n",
      "    21. Student\n",
      "    22. White\n",
      "    23. Black\n",
      "    24. Asian\n",
      "    25. Other_Race\n",
      "    26. Hispanic\n",
      "    27. Bachelor_Above\n",
      "    28. Home_Owner\n",
      "    29. Renter\n",
      "    30. hhsize\n",
      "    31. hhveh\n",
      "    32. Zero_Vehicle_HH\n",
      "    33. hhinc\n",
      "    34. Low_Income\n",
      "    35. Med_Income\n",
      "    36. High_Income\n",
      "    37. dist_rail_ft\n",
      "    38. dist_bus_ft\n",
      "    39. bus_count_14mile\n",
      "    40. dist_rail_mi\n",
      "    41. dist_bus_mi\n",
      "    42. home_x\n",
      "    43. home_y\n",
      "    44. dist_cbd_ft\n",
      "    45. dist_cbd_mi\n",
      "    46. inside_cbd\n",
      "    47. H_intersection_density\n",
      "    48. H_road_network_complexity\n",
      "    49. H_building_density\n",
      "    50. H_land_use_diversity\n",
      "    51. H_amenity_density\n",
      "    52. GEOID_CBG\n",
      "    53. D1B\n",
      "    54. D1C\n",
      "    55. D2A_EPHHM\n",
      "    56. dist_park_mi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"    REMOVE MISSING & NEGATIVE VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Load Data\n",
    "# ============================================================================\n",
    "\n",
    "df = pd.read_csv('./city_home_based_chicago_research_ready_v3.1.csv')\n",
    "print(f\"\\nOriginal data: {len(df):,} observations, {len(df.columns)} variables\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Identify Negative Values (Survey Codes)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 1] Identifying Negative Values / 识别负值\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Columns to check for negative survey codes\n",
    "# These are typically demographic/survey response columns\n",
    "exclude_cols = ['sampno', 'perno', 'GEOID_CBG']\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "check_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "# Count negatives before\n",
    "print(\"\\n  Variables with negative values (before cleaning):\")\n",
    "print(\"  \" + \"-\" * 60)\n",
    "\n",
    "negative_info = {}\n",
    "for col in check_cols:\n",
    "    n_neg = (df[col] < 0).sum()\n",
    "    if n_neg > 0:\n",
    "        min_val = df[col].min()\n",
    "        negative_info[col] = {'count': n_neg, 'min': min_val}\n",
    "        print(f\"    {col:<30} {n_neg:>6,} negatives (min: {min_val})\")\n",
    "\n",
    "print(f\"\\n  Total variables with negatives: {len(negative_info)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Replace Negative Values with NaN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 2] Replacing Negative Values with NaN / 将负值替换为NaN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Replace negative values with NaN for all numeric columns\n",
    "for col in check_cols:\n",
    "    n_before = (df_clean[col] < 0).sum()\n",
    "    if n_before > 0:\n",
    "        df_clean.loc[df_clean[col] < 0, col] = np.nan\n",
    "        print(f\"    {col}: {n_before:,} negative values → NaN\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Count Missing Values After Replacement\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 3] Missing Values Summary (After Negative Removal)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_summary = []\n",
    "for col in check_cols:\n",
    "    n_missing = df_clean[col].isna().sum()\n",
    "    pct_missing = n_missing / len(df_clean) * 100\n",
    "    missing_summary.append({\n",
    "        'Variable': col,\n",
    "        'N_Missing': n_missing,\n",
    "        'Pct_Missing': pct_missing\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary)\n",
    "missing_df = missing_df[missing_df['N_Missing'] > 0].sort_values('N_Missing', ascending=False)\n",
    "\n",
    "print(f\"\\n  {'Variable':<30} {'N_Missing':>10} {'% Missing':>10}\")\n",
    "print(\"  \" + \"-\" * 55)\n",
    "for _, row in missing_df.iterrows():\n",
    "    print(f\"  {row['Variable']:<30} {row['N_Missing']:>10,} {row['Pct_Missing']:>9.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Remove Rows with Any Missing Values\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 4] Removing Rows with Missing Values / 删除含缺失值的行\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_before = len(df_clean)\n",
    "\n",
    "# Drop rows with any missing values in numeric columns\n",
    "df_complete = df_clean.dropna(subset=check_cols)\n",
    "\n",
    "n_after = len(df_complete)\n",
    "n_removed = n_before - n_after\n",
    "pct_removed = n_removed / n_before * 100\n",
    "\n",
    "print(f\"\\n  Before: {n_before:,} observations\")\n",
    "print(f\"  After:  {n_after:,} observations\")\n",
    "print(f\"  Removed: {n_removed:,} observations ({pct_removed:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Verify No Missing or Negative Values\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 5] Verification / 验证\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for any remaining missing values\n",
    "remaining_missing = df_complete[check_cols].isna().sum().sum()\n",
    "print(f\"\\n  Remaining missing values: {remaining_missing}\")\n",
    "\n",
    "# Check for any remaining negative values\n",
    "remaining_negative = (df_complete[check_cols] < 0).sum().sum()\n",
    "print(f\"  Remaining negative values: {remaining_negative}\")\n",
    "\n",
    "if remaining_missing == 0 and remaining_negative == 0:\n",
    "    print(\"\\n  ✓ Dataset is clean - no missing or negative values!\")\n",
    "else:\n",
    "    print(\"\\n  ⚠ Warning: Some issues remain\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Summary Statistics of Clean Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 6] Summary Statistics (Clean Data) / 清洗后统计\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stats_list = []\n",
    "for col in check_cols:\n",
    "    data = df_complete[col]\n",
    "    stats_list.append({\n",
    "        'Variable': col,\n",
    "        'N': len(data),\n",
    "        'Mean': data.mean(),\n",
    "        'Std': data.std(),\n",
    "        'Min': data.min(),\n",
    "        'Median': data.median(),\n",
    "        'Max': data.max()\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_list)\n",
    "\n",
    "print(f\"\\n{'Variable':<30} {'N':>7} {'Mean':>12} {'Std':>12} {'Min':>10} {'Max':>12}\")\n",
    "print(\"=\" * 90)\n",
    "for _, row in stats_df.iterrows():\n",
    "    print(f\"{row['Variable']:<30} {row['N']:>7,} {row['Mean']:>12.4f} \"\n",
    "          f\"{row['Std']:>12.4f} {row['Min']:>10.4f} {row['Max']:>12.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Save Clean Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 7] Save Clean Dataset / 保存清洗后数据\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_file = './city_home_based_chicago_research_ready_v3_clean.csv'\n",
    "df_complete.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n  ✓ Saved: {output_file}\")\n",
    "print(f\"    Rows: {len(df_complete):,}\")\n",
    "print(f\"    Columns: {len(df_complete.columns)}\")\n",
    "\n",
    "# Also save summary statistics\n",
    "stats_df.to_csv('./city_home_based_summary_statistics_clean.csv', index=False)\n",
    "print(f\"  ✓ Saved: ./city_home_based_summary_statistics_clean.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY / 最终总结\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    CLEAN DATASET CREATED                                     ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "  Data Cleaning Summary / 数据清洗总结:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "    Original observations:        {n_before:,}\n",
    "    Clean observations:           {n_after:,}\n",
    "    Removed observations:         {n_removed:,} ({pct_removed:.1f}%)\n",
    "\n",
    "  Cleaning Steps / 清洗步骤:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "    1. Replaced negative values (survey codes -7, -8, -9, etc.) with NaN\n",
    "    2. Removed all rows containing any NaN values\n",
    "\n",
    "  Verification / 验证:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "    Remaining missing values:     {remaining_missing}\n",
    "    Remaining negative values:    {remaining_negative}\n",
    "\n",
    "  Output Files / 输出文件:\n",
    "  ────────────────────────────────────────────────────────────────────────────\n",
    "    ✓ ./chicago_research_ready_v3_clean.csv   (clean dataset)\n",
    "    ✓ ./summary_statistics_clean.csv          (summary stats)\n",
    "\n",
    "══════════════════════════════════════════════════════════════════════════════\n",
    "\"\"\")\n",
    "\n",
    "# Print variable list\n",
    "print(\"\\n  Variables in clean dataset:\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "for i, col in enumerate(df_complete.columns, 1):\n",
    "    print(f\"    {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624a1a8-35ca-41b9-a5fe-697739c4709d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
